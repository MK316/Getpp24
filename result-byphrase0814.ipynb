{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOFGv10hI8lweoKdeKU15kb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Getpp24/blob/main/result-byphrase0814.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data process: split into sentences and save them as list (10PM, 0814)\n",
        "\n",
        "+ data: balanced-toanalyzed (TED. NOW combined)\n",
        "+ final data for write-ups"
      ],
      "metadata": {
        "id": "ArUJv0OZ7sJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "_Rgk3x9-Qm8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split text into sentences and list them under 'Sentlist' column\n",
        "\n",
        "+ Nsent2: new count of number of sentences to check"
      ],
      "metadata": {
        "id": "1PLOZbdjBzco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download the necessary NLTK data for sentence tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/balanced-toanalyze.csv')\n",
        "\n",
        "# Step 1: Split the 'text-only' column into sentences and store them in a new column 'Sentlist'\n",
        "data['Sentlist'] = data['text-only'].apply(sent_tokenize)\n",
        "\n",
        "# Step 2: Count the number of sentences in each list and store in 'Nsent2'\n",
        "data['Nsent2'] = data['Sentlist'].apply(len)\n",
        "\n",
        "# Step 3: Compare 'Nsent' and 'Nsent2' and display IDs where they do not match\n",
        "mismatched_ids = data[data['Nsent'] != data['Nsent2']]['ID']\n",
        "\n",
        "# Display the mismatched IDs\n",
        "if len(mismatched_ids) > 0:\n",
        "    print(\"The following IDs have mismatched sentence counts between 'Nsent' and 'Nsent2':\")\n",
        "    print(mismatched_ids.to_list())\n",
        "else:\n",
        "    print(\"All IDs have matching sentence counts between 'Nsent' and 'Nsent2'.\")\n"
      ],
      "metadata": {
        "id": "r6mmDqYjB4gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('/content/total-sentencelists.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "E7j74ZnKCuBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download the necessary NLTK data for sentence tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/balanced-toanalyze.csv')\n",
        "\n",
        "# Step 1: Split the 'text-only' column into sentences and store them in a new column 'Sentlist'\n",
        "data['Sentlist'] = data['text-only'].apply(sent_tokenize)\n",
        "\n",
        "# Step 2: Count the number of sentences in each list and store in 'Nsent2'\n",
        "data['Nsent2'] = data['Sentlist'].apply(len)\n",
        "\n",
        "# Step 3: Compare 'Nsent' and 'Nsent2' and display rows where they do not match\n",
        "mismatched_data = data[data['Nsent'] != data['Nsent2']].copy()\n",
        "\n",
        "# Add a new column to show the difference between 'Nsent' and 'Nsent2'\n",
        "mismatched_data['Difference'] = mismatched_data['Nsent2'] - mismatched_data['Nsent']\n",
        "\n",
        "# Display the mismatched data with ID, Nsent, Nsent2, and the Difference\n",
        "mismatched_data_df = mismatched_data[['ID', 'Nsent', 'Nsent2', 'Difference']]\n",
        "print(mismatched_data_df)\n"
      ],
      "metadata": {
        "id": "L0a-KYQsCXwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result file: be pp and get pp"
      ],
      "metadata": {
        "id": "xicvC1Ac74lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Necessary Libraries\n",
        "First, ensure you have the required libraries."
      ],
      "metadata": {
        "id": "gMFnplbpF3Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources if not already available\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "w-bwYy0RFv2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define Helper Functions\n",
        "Next, define functions to detect the be + past participle and get + past participle constructions."
      ],
      "metadata": {
        "id": "P8KPaNSnF1FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to identify 'be' + past participle\n",
        "def find_be_passive(sentences):\n",
        "    be_passive_pattern = re.compile(r'\\b(?:is|are|was|were|am|be|been|being|\\'s)\\s+\\b(\\w+ed|\\w+en)\\b', re.IGNORECASE)\n",
        "    return [sentence for sentence in sentences if be_passive_pattern.search(sentence)]\n",
        "\n",
        "# Function to identify 'get' + past participle\n",
        "def find_get_passive(sentences):\n",
        "    get_passive_pattern = re.compile(r'\\bget\\s+\\b(\\w+ed|\\w+en)\\b', re.IGNORECASE)\n",
        "    return [sentence for sentence in sentences if get_passive_pattern.search(sentence)]\n"
      ],
      "metadata": {
        "id": "i2zDqw6AFxxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Load Data and Process Sentences\n",
        "Load your data, process it to find passive sentences, and create new columns accordingly."
      ],
      "metadata": {
        "id": "43CVYayVF48i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('/content/total-sentencelists.csv')\n",
        "\n",
        "# Process each list of sentences\n",
        "data['Bepp'] = data['Sentlist'].apply(lambda x: find_be_passive(eval(x)))\n",
        "data['Getpp'] = data['Sentlist'].apply(lambda x: find_get_passive(eval(x)))\n",
        "\n",
        "# Count the number of sentences for each passive form\n",
        "data['Nbepp'] = data['Bepp'].apply(len)\n",
        "data['Ngetpp'] = data['Getpp'].apply(len)\n"
      ],
      "metadata": {
        "id": "AsGqbSHoFzn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Save the Final DataFrame\n",
        "Finally, save the processed data to a new CSV file."
      ],
      "metadata": {
        "id": "NizijGmuGACq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final data to a CSV file\n",
        "data.to_csv('/content/02-total-voice-count.csv', index=False)\n",
        "\n",
        "print(\"Processing complete. The data has been saved to '/content/02-total-voice-count.csv'.\")\n"
      ],
      "metadata": {
        "id": "jFGzhBu_GAmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S808 text has errors and this file is processed separately"
      ],
      "metadata": {
        "id": "pJC7yLdgMvhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown S808 processed and saved already (voicetotal.csv)\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download NLTK resources if not already available\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to identify 'be' + past participle\n",
        "def find_be_passive(sentences):\n",
        "    be_passive_pattern = re.compile(r'\\b(?:is|are|was|were|am|be|been|being|\\'s)\\s+\\b(\\w+ed|\\w+en)\\b', re.IGNORECASE)\n",
        "    return [sentence for sentence in sentences if be_passive_pattern.search(sentence)]\n",
        "\n",
        "# Function to identify 'get' + past participle\n",
        "def find_get_passive(sentences):\n",
        "    get_passive_pattern = re.compile(r'\\bget\\s+\\b(\\w+ed|\\w+en)\\b', re.IGNORECASE)\n",
        "    return [sentence for sentence in sentences if get_passive_pattern.search(sentence)]\n",
        "\n",
        "# Load the text from the file\n",
        "with open('S808.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Split the text into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Find be-passive and get-passive sentences\n",
        "be_passive_sentences = find_be_passive(sentences)\n",
        "get_passive_sentences = find_get_passive(sentences)\n",
        "\n",
        "# Count the number of sentences in each list\n",
        "n_be_passive = len(be_passive_sentences)\n",
        "n_get_passive = len(get_passive_sentences)\n",
        "\n",
        "# Save the lists of sentences to files\n",
        "with open('be_passive_sentences.txt', 'w', encoding='utf-8') as be_file:\n",
        "    for sentence in be_passive_sentences:\n",
        "        be_file.write(sentence + '\\n')\n",
        "\n",
        "with open('get_passive_sentences.txt', 'w', encoding='utf-8') as get_file:\n",
        "    for sentence in get_passive_sentences:\n",
        "        get_file.write(sentence + '\\n')\n",
        "\n",
        "# Display the results\n",
        "print(\"Be-Passive Sentences:\")\n",
        "for sentence in be_passive_sentences:\n",
        "    print(sentence)\n",
        "print(f\"\\nTotal Be-Passive Sentences: {n_be_passive}\")\n",
        "\n",
        "print(\"\\nGet-Passive Sentences:\")\n",
        "for sentence in get_passive_sentences:\n",
        "    print(sentence)\n",
        "print(f\"\\nTotal Get-Passive Sentences: {n_get_passive}\")\n",
        "\n",
        "print(\"The sentences have been saved to 'be_passive_sentences.txt' and 'get_passive_sentences.txt'.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kTmQVY11M1xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result descriptive statistics\n",
        "\n",
        "+ Voice result: [data](https://drive.google.com/file/d/1noN8GMQuCaGgQHQKnsM_UDSvkYdRgCeB/view?usp=sharing)\n",
        "+ Total result: Be pp and Get pp counts (0814)"
      ],
      "metadata": {
        "id": "LpVG1jPsKM-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/file/d/1noN8GMQuCaGgQHQKnsM_UDSvkYdRgCeB/view?usp=sharing\""
      ],
      "metadata": {
        "id": "gIBYpfxeKR_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Step 1: Read the CSV file from the provided URL\n",
        "url = \"/content/voicetotal.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Display the column names\n",
        "print(\"Column Names in the DataFrame:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Step 3: Define a function to check for 'by' phrases in a list of sentences\n",
        "def find_by_phrase(sentences):\n",
        "    by_phrase_pattern = re.compile(r'\\bby\\b', re.IGNORECASE)\n",
        "    return [sentence for sentence in sentences if by_phrase_pattern.search(sentence)]\n",
        "\n",
        "# Step 4: Process 'Bepp' and 'Getpp' columns to find sentences with 'by' phrases\n",
        "df['Bepp-by'] = df['Bepp'].apply(lambda x: find_by_phrase(eval(x)))\n",
        "df['Getpp-by'] = df['Getpp'].apply(lambda x: find_by_phrase(eval(x)))\n",
        "\n",
        "# Step 5: Count the number of sentences containing 'by' phrases\n",
        "df['NBepp-by'] = df['Bepp-by'].apply(len)\n",
        "df['NGetpp-by'] = df['Getpp-by'].apply(len)\n",
        "\n",
        "# Step 6: Save the final DataFrame to a CSV file\n",
        "output_path = '/content/03-results-byphrase.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"The final DataFrame has been saved to '{output_path}'.\")\n"
      ],
      "metadata": {
        "id": "L_rFJGUiR_fU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}