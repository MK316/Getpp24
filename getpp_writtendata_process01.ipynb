{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMu8oLf9sfa9TjFaMKasbvK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Getpp24/blob/main/getpp_writtendata_process01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Written text processing (0811)\n",
        "\n",
        "The output is saved as 'getpp-written.xlsx' with a log file\n",
        "\n",
        "+ Input: getpp-written.txt [link](https://github.com/MK316/Getpp24/blob/main/data/getpp-written.txt)"
      ],
      "metadata": {
        "id": "xyOXzFMo2e2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove the initial ID from the text\n",
        "    text = re.sub(r'^@@\\d+\\s*', '', text)\n",
        "\n",
        "    # Replace corrupted encoding sequences\n",
        "    text = re.sub(r'\\*\\*\\d+;\\d+;[^\\s]+', '(brokenencoding)', text)\n",
        "\n",
        "    # Remove content within <h> tags until the first <p> tag\n",
        "    text = re.sub(r'<h>.*?<p>', '<p>', text, flags=re.DOTALL)\n",
        "\n",
        "    # Remove all <p> tags, but keep the content\n",
        "    text = re.sub(r'<\\/?p>', '', text)\n",
        "\n",
        "    # Remove sequences of '@' characters possibly with spaces\n",
        "    text = re.sub(r'(@\\s+)+@', ' ', text)  # Replaces sequences of '@' with a single space\n",
        "    text = re.sub(r'@+', ' ', text)  # Replaces remaining '@' characters\n",
        "\n",
        "    # Remove space before commas, periods, or any common punctuation\n",
        "    text = re.sub(r'\\s+(?=[,.!?;:])', '', text)\n",
        "\n",
        "    # Correctly handle contractions\n",
        "    contractions = {\n",
        "        r\"(\\b[a-zA-Z]+) 's\\b\": r\"\\1's\",\n",
        "        r\"(\\b[a-zA-Z]+) 'nt\\b\": r\"\\1n't\",\n",
        "        r\"(\\b[a-zA-Z]+) 'm\\b\": r\"\\1'm\",\n",
        "        r\"(\\b[a-zA-Z]+) 're\\b\": r\"\\1're\",\n",
        "        r\"(\\b[a-zA-Z]+) 've\\b\": r\"\\1've\",\n",
        "        r\"(\\b[a-zA-Z]+) 'd\\b\": r\"\\1'd\",\n",
        "        r\"(\\b[a-zA-Z]+) 'll\\b\": r\"\\1'll\"\n",
        "    }\n",
        "    for pattern, replacement in contractions.items():\n",
        "        text = re.sub(pattern, replacement, text)\n",
        "\n",
        "    # Remove spaces inside single and double quotes\n",
        "    text = re.sub(r\"' (\\S.*?\\S) '\", r\"'\\1'\", text)\n",
        "    text = re.sub(r'\" (\\S.*?\\S) \"', r'\"\\1\"', text)\n",
        "\n",
        "    # Ensure multiple spaces are reduced to a single space\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def process_file(input_path, output_path):\n",
        "    data = []  # To store the results\n",
        "    # Open the file and read line by line\n",
        "    with open(input_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            # Extract ID using regex\n",
        "            id_match = re.search(r'@@(\\d+)', line)\n",
        "            if id_match:\n",
        "                id = id_match.group(1)  # Capture the numeric part of the ID\n",
        "\n",
        "                # Apply text cleaning to the line\n",
        "                cleaned_text = clean_text(line.strip())\n",
        "\n",
        "                # Append the cleaned text and ID to the data list\n",
        "                data.append({'ID': id, 'Text': cleaned_text})\n",
        "\n",
        "    # Convert list of dictionaries to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Output saved to {output_path}\")\n",
        "\n",
        "# Specify the paths\n",
        "input_path = 'getpp-written.txt'\n",
        "output_path = 'getpp-written.csv'\n",
        "\n",
        "# Process the file\n",
        "process_file(input_path, output_path)\n"
      ],
      "metadata": {
        "id": "3HpUHMUQ5zw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] Getpp file information\n",
        "\n",
        "Nchar, Nword, Nsent in separate columns"
      ],
      "metadata": {
        "id": "7rnnPvv4B19X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Ensure that NLTK's tokenizers are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "def add_text_statistics(df):\n",
        "    # Define functions to calculate the number of characters, words, and sentences\n",
        "    def count_chars(text):\n",
        "        return len(text)\n",
        "\n",
        "    def count_words(text):\n",
        "        words = word_tokenize(text)\n",
        "        return len(words)\n",
        "\n",
        "    def count_sentences(text):\n",
        "        sentences = sent_tokenize(text)\n",
        "        return len(sentences)\n",
        "\n",
        "    # Apply these functions to the 'Text' column\n",
        "    df['Nchar'] = df['Text'].apply(count_chars)\n",
        "    df['Nword'] = df['Text'].apply(count_words)\n",
        "    df['Nsent'] = df['Text'].apply(count_sentences)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load your existing CSV file\n",
        "input_path = '/content/getpp-written00.csv'  # Adjust this path to your actual file\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Add the text statistics to the DataFrame\n",
        "df = add_text_statistics(df)\n",
        "\n",
        "# Save the enhanced DataFrame to a new CSV file\n",
        "output_path = '/content/getpp-written-info.csv'  # Adjust the output path if needed\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Enhanced DataFrame saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "zz-qNcCxB9FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your existing CSV file\n",
        "input_path = '/content/getpp-written-info.csv'  # Adjust this path to your actual file\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Calculate the sums of the Nchar, Nword, and Nsent columns\n",
        "total_nchar = df['Nchar'].sum()\n",
        "total_nword = df['Nword'].sum()\n",
        "total_nsent = df['Nsent'].sum()\n",
        "\n",
        "# Display the results\n",
        "print(f\"Total Number of Characters (Nchar): {total_nchar}\")\n",
        "print(f\"Total Number of Words (Nword): {total_nword}\")\n",
        "print(f\"Total Number of Sentences (Nsent): {total_nsent}\")\n"
      ],
      "metadata": {
        "id": "lxQyBgZsC4cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add average length of word and sentence"
      ],
      "metadata": {
        "id": "sGGPbIW2D3uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Ensure that NLTK's tokenizers are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "def add_text_statistics(df):\n",
        "    # Define functions to calculate the number of characters, words, and sentences\n",
        "    def count_chars(text):\n",
        "        return len(text)\n",
        "\n",
        "    def count_words(text):\n",
        "        words = word_tokenize(text)\n",
        "        return len(words)\n",
        "\n",
        "    def count_sentences(text):\n",
        "        sentences = sent_tokenize(text)\n",
        "        return len(sentences)\n",
        "\n",
        "    # Apply these functions to the 'Text' column\n",
        "    df['Nchar'] = df['Text'].apply(count_chars)\n",
        "    df['Nword'] = df['Text'].apply(count_words)\n",
        "    df['Nsent'] = df['Text'].apply(count_sentences)\n",
        "\n",
        "    # Calculate average number of words per sentence (AVG-word)\n",
        "    df['AVG-word'] = df.apply(lambda row: row['Nword'] / row['Nsent'] if row['Nsent'] > 0 else 0, axis=1)\n",
        "\n",
        "    # Calculate average number of characters per word (AVG-sent)\n",
        "    df['AVG-sent'] = df.apply(lambda row: row['Nchar'] / row['Nword'] if row['Nword'] > 0 else 0, axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load your existing CSV file\n",
        "input_path = '/content/getpp-written-info.csv'  # Adjust this path to your actual file\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Add the text statistics to the DataFrame\n",
        "df = add_text_statistics(df)\n",
        "\n",
        "# Calculate and display the sums of Nchar, Nword, and Nsent\n",
        "total_nchar = df['Nchar'].sum()\n",
        "total_nword = df['Nword'].sum()\n",
        "total_nsent = df['Nsent'].sum()\n",
        "\n",
        "print(f\"Total Number of Characters (Nchar): {total_nchar}\")\n",
        "print(f\"Total Number of Words (Nword): {total_nword}\")\n",
        "print(f\"Total Number of Sentences (Nsent): {total_nsent}\")\n",
        "\n",
        "# Save the enhanced DataFrame to a new CSV file\n",
        "output_path = '/content/getpp-written-info.csv'  # Adjust the output path if needed\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Enhanced DataFrame saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "W5qXYovOD7un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "RW6fQhQMD-pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total average word length and sentence length"
      ],
      "metadata": {
        "id": "hpnSPc8dEOor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Ensure that NLTK's tokenizers are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "def add_text_statistics(df):\n",
        "    # Define functions to calculate the number of characters, words, and sentences\n",
        "    def count_chars(text):\n",
        "        return len(text)\n",
        "\n",
        "    def count_words(text):\n",
        "        words = word_tokenize(text)\n",
        "        return len(words)\n",
        "\n",
        "    def count_sentences(text):\n",
        "        sentences = sent_tokenize(text)\n",
        "        return len(sentences)\n",
        "\n",
        "    # Apply these functions to the 'Text' column\n",
        "    df['Nchar'] = df['Text'].apply(count_chars)\n",
        "    df['Nword'] = df['Text'].apply(count_words)\n",
        "    df['Nsent'] = df['Text'].apply(count_sentences)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load your existing CSV file\n",
        "input_path = '/content/getpp-written-info.csv'  # Adjust this path to your actual file\n",
        "df = pd.read_csv(input_path)\n",
        "\n",
        "# Add the text statistics to the DataFrame\n",
        "df = add_text_statistics(df)\n",
        "\n",
        "# Calculate the sums of Nchar, Nword, and Nsent\n",
        "total_nchar = df['Nchar'].sum()\n",
        "total_nword = df['Nword'].sum()\n",
        "total_nsent = df['Nsent'].sum()\n",
        "\n",
        "# Calculate the overall average word length (total Nchar / total Nword)\n",
        "avg_word_length = total_nchar / total_nword if total_nword > 0 else 0\n",
        "\n",
        "# Calculate the overall average sentence length (total Nword / total Nsent)\n",
        "avg_sentence_length = total_nword / total_nsent if total_nsent > 0 else 0\n",
        "\n",
        "# Display the results\n",
        "print(f\"Total Number of Characters (Nchar): {total_nchar}\")\n",
        "print(f\"Total Number of Words (Nword): {total_nword}\")\n",
        "print(f\"Total Number of Sentences (Nsent): {total_nsent}\")\n",
        "print(f\"Average Word Length: {avg_word_length:.2f} characters per word\")\n",
        "print(f\"Average Sentence Length: {avg_sentence_length:.2f} words per sentence\")\n",
        "\n",
        "# Save the enhanced DataFrame to a new CSV file\n",
        "output_path = '/content/getpp-written-info.csv'  # Adjust the output path if needed\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Enhanced DataFrame saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "80J9ymfwERps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}