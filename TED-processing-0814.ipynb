{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOV0nXvIuSbIUC7bDaQ54rl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Getpp24/blob/main/TED-processing-0814.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v90dSNrddXgP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/wresult02.csv')\n",
        "\n",
        "# Display descriptive statistics for 'Nword' and 'Nsent' columns\n",
        "descriptive_stats = df[['Nword', 'Nsent']].describe()\n",
        "print(descriptive_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# year count\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/wresult02.csv')\n",
        "\n",
        "# Ensure the 'year' column is of string type\n",
        "df['year'] = df['year'].astype(str)\n",
        "\n",
        "# Get the counts of each year\n",
        "year_counts = df['year'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(year_counts)\n"
      ],
      "metadata": {
        "id": "feoGcYnZl7G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TED selection"
      ],
      "metadata": {
        "id": "8_j-w0130D4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Specify the path to your file (you can skip this if you're not using Google Drive)\n",
        "file_path = '/content/ted-part01.txt'\n",
        "\n",
        "\n",
        "# Step 3: Read the text file\n",
        "with open(file_path, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Step 4: Count the number of words\n",
        "word_count = len(text.split())\n",
        "\n",
        "# Step 5: Count the number of sentences\n",
        "import re\n",
        "sentence_count = len(re.split(r'[.!?]+', text)) - 1\n",
        "\n",
        "# Step 6: Print the results\n",
        "print(f'Total number of words: {word_count}')\n",
        "print(f'Total number of sentences: {sentence_count}')\n"
      ],
      "metadata": {
        "id": "mK6X2Cn90GFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TED 1760 file\n",
        "\n",
        "+ remove ( ), [ ], then save the text under 'text-only' column\n",
        "+ split n't, 're, 've, 'd, and then count words and sentences"
      ],
      "metadata": {
        "id": "I2Mi3mOF5tmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "\n",
        "# Make sure NLTK's punkt tokenizer models are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 1) Read the data with encoding 'utf-8'\n",
        "file_path = '/content/selected1760.csv'  # Update this path if needed\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "# 2) Remove any string within () or [] and save in a new column 'text-only'\n",
        "df['text-only'] = df['transcript'].apply(lambda x: re.sub(r'\\(.*?\\)|\\[.*?\\]', ' ', x))\n",
        "\n",
        "# 3) For each text in 'text-only', split 'nt, 've, 're and count words, record in 'Nword'\n",
        "def clean_and_count_words(text):\n",
        "    # Handle contractions\n",
        "    text = re.sub(r\"(?<=\\w)'t\", \" 't\", text)  # handle 'nt\n",
        "    text = re.sub(r\"(?<=\\w)'ve\", \" 've\", text)  # handle 've\n",
        "    text = re.sub(r\"(?<=\\w)'re\", \" 're\", text)  # handle 're\n",
        "    text = re.sub(r\"(?<=\\w)'d\", \" 'd\", text)  # handle 'd\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Filter out tokens that consist only of punctuation\n",
        "    words = [word for word in words if word not in string.punctuation]\n",
        "\n",
        "    return len(words)\n",
        "\n",
        "# Apply the function to the 'text-only' column\n",
        "df['Nword'] = df['text-only'].apply(clean_and_count_words)\n",
        "\n",
        "# 4) Count sentences and record in 'Nsent'\n",
        "df['Nsent'] = df['text-only'].apply(lambda x: len(sent_tokenize(x)))\n",
        "\n",
        "# 5) Calculate the total number of words and sentences\n",
        "total_words = df['Nword'].sum()\n",
        "total_sentences = df['Nsent'].sum()\n",
        "\n",
        "# Display the totals\n",
        "print(f'Total number of words: {total_words}')\n",
        "print(f'Total number of sentences: {total_sentences}')\n",
        "\n",
        "df.to_csv('/content/s1760-processed.csv', encoding='utf-8', index=False)\n",
        "# Optionally, display the first few rows of the dataframe to verify\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "pnmhuTja57fv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}