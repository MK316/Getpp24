{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM5wFxfpwWnBrSxqof+XY/c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Getpp24/blob/main/wdata_0812.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# wdata process (7PM, 0812)"
      ],
      "metadata": {
        "id": "ArUJv0OZ7sJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Step 01: text file to csv\n",
        "\n",
        "+ Cautions to make: two files are too big and they will be split into two files: **1471241a, 1471241b; 8671240a, 8671240b**\n",
        "+ input file: getpp-written.txt\n",
        "+ output file: wdata00.csv (with ID, Text columns)\n",
        "+ 2916-2 files (2914 files)"
      ],
      "metadata": {
        "id": "RE9Jb28-i4ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Ensure that the necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# File path for the input and output files\n",
        "input_file = '/content/getpp-written.txt'  # Replace with your actual file path\n",
        "output_file = '/content/wdata00.csv'\n",
        "\n",
        "# Initialize a list to store the processed data\n",
        "data = []\n",
        "\n",
        "# Read the text file\n",
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Process each line\n",
        "for line in lines:\n",
        "    # Remove leading '@' symbols and split into ID and Text\n",
        "    line = line.strip()\n",
        "    id_text_split = re.split(r'\\s+', line, maxsplit=1)\n",
        "\n",
        "    if len(id_text_split) < 2:\n",
        "        continue  # Skip if the line doesn't have both ID and Text parts\n",
        "\n",
        "    ID = id_text_split[0].replace('@', '')\n",
        "    text = id_text_split[1]\n",
        "\n",
        "    # Check for specific IDs that require splitting\n",
        "    if ID in ['1471241', '8671240']:\n",
        "        # Split the text into sentences\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        mid_point = len(sentences) // 2\n",
        "\n",
        "        # Join the sentences to form two parts\n",
        "        text_part1 = ' '.join(sentences[:mid_point])\n",
        "        text_part2 = ' '.join(sentences[mid_point:])\n",
        "\n",
        "        # Append both parts with 'a' and 'b' appended to the ID\n",
        "        data.append([f'{ID}a', text_part1])\n",
        "        data.append([f'{ID}b', text_part2])\n",
        "    else:\n",
        "        # Append the normal ID and Text\n",
        "        data.append([ID, text])\n",
        "\n",
        "# Convert the list to a DataFrame\n",
        "df = pd.DataFrame(data, columns=['ID', 'Text'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(output_file, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Processing complete. Data saved to {output_file}.\")\n"
      ],
      "metadata": {
        "id": "3cPVCM7mjMrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Tag to remove"
      ],
      "metadata": {
        "id": "8CKfzZNjnvLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Ensure that the necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the existing CSV file\n",
        "file_path = '/content/wdata00.csv'  # Update this to the path of your actual file\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "# Function to remove <h> and <p> tags and count words\n",
        "def process_text(text):\n",
        "    # Remove <h> and <p> tags\n",
        "    clean_text = re.sub(r'<\\/?[hp]>', '', text)\n",
        "\n",
        "    # Tokenize text to count words, excluding @ symbols and symbol-only strings\n",
        "    tokens = nltk.word_tokenize(clean_text)\n",
        "    word_count = sum(1 for token in tokens if token.isalnum())\n",
        "\n",
        "    return clean_text, word_count\n",
        "\n",
        "# Apply processing to each row\n",
        "df['Text'], df['Nword'] = zip(*df['Text'].apply(process_text))\n",
        "\n",
        "# Save the modified DataFrame back to the CSV file\n",
        "output_path = '/content/wdata01.csv'\n",
        "df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Processing complete. Data saved to {file_path}.\")\n"
      ],
      "metadata": {
        "id": "vV9AzsTun1X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] White space to remove before punctuations"
      ],
      "metadata": {
        "id": "rtNthubCrXXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Remove the space before punctuation and save the cleaned text in a new column text"
      ],
      "metadata": {
        "id": "N2ZVvv2Tsdr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the existing CSV file\n",
        "file_path = '/content/wdata01.csv'  # Update this to the path of your actual file\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "# Function to remove spaces before punctuation\n",
        "def remove_space_before_punctuation(text):\n",
        "    # Remove space before any punctuation\n",
        "    cleaned_text = re.sub(r'\\s+([?.!\",:;])', r'\\1', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Apply the function to remove spaces before punctuation and create a new column 'text'\n",
        "df['text'] = df['Text'].apply(remove_space_before_punctuation)\n",
        "\n",
        "# Save the DataFrame with the new 'text' column\n",
        "df.to_csv('/content/wdata02_step1.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Step 1 complete. Data saved with cleaned text to 'wdata02_step1.csv'.\")\n"
      ],
      "metadata": {
        "id": "U7toNUv9scrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Split the text by sentences, count the number of sentences, and filter out sentences that are only symbols/punctuation"
      ],
      "metadata": {
        "id": "7Du2-80RshmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the CSV file with the cleaned text\n",
        "df = pd.read_csv('/content/wdata02_step1.csv', encoding='utf-8')\n",
        "\n",
        "# Function to split text into sentences, filter out non-word sentences, and count them\n",
        "def split_and_count_sentences(text):\n",
        "    # Split text into sentences\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # Filter out sentences that contain only symbols or punctuation\n",
        "    valid_sentences = [sent for sent in sentences if any(word.isalnum() for word in nltk.word_tokenize(sent))]\n",
        "\n",
        "    # Count the number of valid sentences\n",
        "    sentence_count = len(valid_sentences)\n",
        "\n",
        "    return valid_sentences, sentence_count\n",
        "\n",
        "# Apply the function to split and count sentences, and save results in 'Sentences' and 'Nsent' columns\n",
        "df['Sentences'], df['Nsent'] = zip(*df['text'].apply(split_and_count_sentences))\n",
        "\n",
        "# Save the DataFrame with the new 'Sentences' and 'Nsent' columns\n",
        "df.to_csv('/content/wdata02.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Step 2 complete. Data saved with sentences and sentence count to 'wdata02.csv'.\")\n"
      ],
      "metadata": {
        "id": "LAaWCfriskDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [4] Passive sentences: count and list\n",
        "\n",
        "+ input: wdata02.csv\n",
        "+ output: wdata03.csv"
      ],
      "metadata": {
        "id": "PujnjuCJuTzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of rows: {len(df)}\")\n",
        "print(f\"Total number of sentences: {df['Sentences'].apply(len).sum()}\")\n"
      ],
      "metadata": {
        "id": "CZz5sT7XxrGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Ensure that the necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/wdata02.csv'  # Update this to the path of your actual file\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip', encoding='utf-8')\n",
        "\n",
        "# Function to detect past participles\n",
        "def is_past_participle(word):\n",
        "    pos = pos_tag([word])[0][1]\n",
        "    return pos == 'VBN'\n",
        "\n",
        "# Function to detect passive voice with 'be' verb\n",
        "def contains_bepp(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tagged = pos_tag(tokens)\n",
        "    for i in range(len(tagged) - 1):\n",
        "        if tagged[i][0].lower() in ['is', 'are', 'was', 'were', 'be', 'been', 'being'] and is_past_participle(tagged[i + 1][0]):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to detect passive voice with 'get' verb\n",
        "def contains_getpp(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tagged = pos_tag(tokens)\n",
        "    for i in range(len(tagged) - 1):\n",
        "        if tagged[i][0].lower() == 'get' and is_past_participle(tagged[i + 1][0]):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to find passives in a list of sentences\n",
        "def find_passives(sentences):\n",
        "    passive_sentences = []\n",
        "    bepp_count = 0\n",
        "    getpp_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if contains_bepp(sentence):\n",
        "            bepp_count += 1\n",
        "            passive_sentences.append(sentence)\n",
        "        elif contains_getpp(sentence):\n",
        "            getpp_count += 1\n",
        "            passive_sentences.append(sentence)\n",
        "\n",
        "    return passive_sentences, bepp_count, getpp_count\n",
        "\n",
        "# Process in batches\n",
        "batch_size = 100  # You can adjust the batch size according to your system's capacity\n",
        "\n",
        "# Initialize a file for storing results\n",
        "output_path = '/content/wdata03.csv'\n",
        "header = True  # To include the header only once\n",
        "\n",
        "for start in range(0, len(df), batch_size):\n",
        "    end = start + batch_size\n",
        "    df_batch = df.iloc[start:end].copy()  # Create a copy of the batch to work on\n",
        "\n",
        "    # Process the batch\n",
        "    df_batch['Passives'], df_batch['Bepp'], df_batch['Getpp'] = zip(*df_batch['Sentences'].apply(lambda x: find_passives(eval(x))))\n",
        "\n",
        "    # Append the processed batch to the output file\n",
        "    df_batch.to_csv(output_path, mode='a', header=header, index=False, encoding='utf-8')\n",
        "\n",
        "    # After the first write, subsequent writes should not include the header\n",
        "    header = False\n",
        "\n",
        "    print(f\"Processed rows {start} to {end}\")\n",
        "\n",
        "print(\"Processing complete and saved to 'wdata03.csv'.\")\n"
      ],
      "metadata": {
        "id": "UncGeDPsyLn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [5] Results: Getting bepp and getpp list and counts."
      ],
      "metadata": {
        "id": "0JgvI0hdz2mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Ensure that the necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/wdata02.csv'  # Update this to the path of your actual file\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip', encoding='utf-8')\n",
        "\n",
        "# Function to detect past participles\n",
        "def is_past_participle(word):\n",
        "    pos = pos_tag([word])[0][1]\n",
        "    return pos == 'VBN'\n",
        "\n",
        "# Function to detect passive voice with 'be' verb\n",
        "def contains_bepp(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tagged = pos_tag(tokens)\n",
        "    for i in range(len(tagged) - 1):\n",
        "        if tagged[i][0].lower() in ['is', 'are', 'was', 'were', 'be', 'been', 'being'] and is_past_participle(tagged[i + 1][0]):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to detect passive voice with 'get' verb\n",
        "def contains_getpp(sentence):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tagged = pos_tag(tokens)\n",
        "    for i in range(len(tagged) - 1):\n",
        "        if tagged[i][0].lower() == 'get' and is_past_participle(tagged[i + 1][0]):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Function to find passives in a list of sentences and separate them\n",
        "def find_passives(sentences):\n",
        "    be_passive_sentences = []\n",
        "    get_passive_sentences = []\n",
        "    bepp_count = 0\n",
        "    getpp_count = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if contains_bepp(sentence):\n",
        "            bepp_count += 1\n",
        "            be_passive_sentences.append(sentence)\n",
        "        elif contains_getpp(sentence):\n",
        "            getpp_count += 1\n",
        "            get_passive_sentences.append(sentence)\n",
        "\n",
        "    return be_passive_sentences, get_passive_sentences, bepp_count, getpp_count\n",
        "\n",
        "# Process in batches\n",
        "batch_size = 100  # You can adjust the batch size according to your system's capacity\n",
        "\n",
        "# Initialize a file for storing results\n",
        "output_path = '/content/passive-result-w01.csv'\n",
        "header = True  # To include the header only once\n",
        "\n",
        "for start in range(0, len(df), batch_size):\n",
        "    end = start + batch_size\n",
        "    df_batch = df.iloc[start:end].copy()  # Create a copy of the batch to work on\n",
        "\n",
        "    # Process the batch\n",
        "    df_batch['BePassive'], df_batch['GetPassive'], df_batch['Bepp'], df_batch['Getpp'] = zip(*df_batch['Sentences'].apply(lambda x: find_passives(eval(x))))\n",
        "\n",
        "    # Append the processed batch to the output file\n",
        "    df_batch.to_csv(output_path, mode='a', header=header, index=False, encoding='utf-8')\n",
        "\n",
        "    # After the first write, subsequent writes should not include the header\n",
        "    header = False\n",
        "\n",
        "    print(f\"Processed rows {start} to {end}\")\n",
        "\n",
        "print(\"Processing complete and saved to 'passive-result-w01.csv'.\")\n"
      ],
      "metadata": {
        "id": "LWWBbEKWz8tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [6] Descriptive statistics"
      ],
      "metadata": {
        "id": "3iHM1Et22S6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the processed dataset\n",
        "file_path = '/content/passive-result-w01.csv'  # Update this to the path of your actual file\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "# Calculate the total counts for Bepp and Getpp\n",
        "total_bepp = df['Bepp'].sum()\n",
        "total_getpp = df['Getpp'].sum()\n",
        "\n",
        "# Generate a summary DataFrame\n",
        "summary_df = pd.DataFrame({\n",
        "    'Type': ['Bepp', 'Getpp'],\n",
        "    'Total Count': [total_bepp, total_getpp]\n",
        "})\n",
        "\n",
        "# Display the summary\n",
        "print(summary_df)\n",
        "\n",
        "# Optionally, save the summary to a new CSV file\n",
        "summary_output_path = '/content/passive_summary.csv'  # Update this to the desired output file path\n",
        "summary_df.to_csv(summary_output_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"Summary saved to '{summary_output_path}'\")\n"
      ],
      "metadata": {
        "id": "x9Dgpe2H2Vp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II. Data analysis"
      ],
      "metadata": {
        "id": "xicvC1Ac74lH"
      }
    }
  ]
}