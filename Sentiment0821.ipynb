{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPzObLXB4rU633RO+Kr2cVy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Getpp24/blob/main/Sentiment0821.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ± GetPP-Sentiment analysis (0821 9PM)"
      ],
      "metadata": {
        "id": "G5u3q_Kh6mdQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWLpFaFzIEgP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Getpp24/main/data/resultall0815-light.csv\"\n",
        "\n",
        "# Step 1: Read the dataframe from the provided URL\n",
        "df = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "lvj-HbXaINc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check cells of 'Bepp' and 'Getpp' whether they are in lists"
      ],
      "metadata": {
        "id": "kE8Wm4UbNfI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check the datatype of each cell in 'Bepp' and display row numbers if the cell is not a list\n",
        "non_list_rows = df[~df['Bepp'].apply(lambda x: isinstance(x, list))].index\n",
        "\n",
        "# Output the row numbers where 'Bepp' is not a list\n",
        "if not non_list_rows.empty:\n",
        "    print(\"Rows where 'Bepp' is not a list:\")\n",
        "    print(non_list_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Bepp' column are lists.\")\n"
      ],
      "metadata": {
        "id": "nAzxcwyIKz2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 'Bepp' cell as lists"
      ],
      "metadata": {
        "id": "iV2-WqwrLb3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "\n",
        "# Function to check and convert strings that look like lists into actual lists\n",
        "def convert_to_list_if_needed(cell):\n",
        "    if isinstance(cell, str):\n",
        "        try:\n",
        "            # Try to convert it to a list\n",
        "            return ast.literal_eval(cell)\n",
        "        except (ValueError, SyntaxError):\n",
        "            # If it fails, return the original string\n",
        "            return cell\n",
        "    return cell\n",
        "\n",
        "# Apply the function to both 'Bepp' and 'Getpp' columns\n",
        "df['Bepp'] = df['Bepp'].apply(convert_to_list_if_needed)\n",
        "df['Getpp'] = df['Getpp'].apply(convert_to_list_if_needed)\n",
        "\n",
        "# Check the datatype of each cell in 'Bepp' and 'Getpp' and display row numbers if the cell is not a list\n",
        "non_list_bepp_rows = df[~df['Bepp'].apply(lambda x: isinstance(x, list))].index\n",
        "non_list_getpp_rows = df[~df['Getpp'].apply(lambda x: isinstance(x, list))].index\n",
        "\n",
        "# Output the row numbers where 'Bepp' is not a list\n",
        "if not non_list_bepp_rows.empty:\n",
        "    print(\"Rows where 'Bepp' is not a list:\")\n",
        "    print(non_list_bepp_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Bepp' column are lists.\")\n",
        "\n",
        "# Output the row numbers where 'Getpp' is not a list\n",
        "if not non_list_getpp_rows.empty:\n",
        "    print(\"Rows where 'Getpp' is not a list:\")\n",
        "    print(non_list_getpp_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Getpp' column are lists.\")\n"
      ],
      "metadata": {
        "id": "a3R8nLsRLOi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bepp-spoken vs. Bepp-written: two lists of sentences"
      ],
      "metadata": {
        "id": "YKm4hiV9JF2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check DataFrame\n",
        "# print(\"Original DataFrame:\")\n",
        "# print(df)\n",
        "\n",
        "# Filter and combine sentence lists based on 'register'\n",
        "bepp_spoken = sum(df[df['register'] == 'Spoken']['Bepp'], [])\n",
        "bepp_written = sum(df[df['register'] == 'Written']['Bepp'], [])\n",
        "\n",
        "getpp_spoken = sum(df[df['register'] == 'Spoken']['Getpp'], [])\n",
        "getpp_written = sum(df[df['register'] == 'Written']['Getpp'], [])\n",
        "# Display the combined lists\n",
        "print(\"\\nCombined Sentences for Spoken Register: Be pp\")\n",
        "print(len(bepp_spoken), bepp_spoken)\n",
        "print(\"\\nCombined Sentences for Written Register: Be pp\")\n",
        "print(len(bepp_written), bepp_written)\n",
        "\n",
        "print(\"\\nCombined Sentences for Spoken Register: Get pp\")\n",
        "print(len(getpp_spoken), getpp_spoken)\n",
        "print(\"\\nCombined Sentences for Written Register: Get pp\")\n",
        "print(len(getpp_written), getpp_written)\n"
      ],
      "metadata": {
        "id": "l15ZKPxAJLTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis with 4 lists\n",
        "\n",
        "+ bepp_spoken, _written\n",
        "+ getpp_spoken, _written"
      ],
      "metadata": {
        "id": "3HJ851-aNlq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "0aGcvNCCLXd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('opinion_lexicon')\n",
        "nltk.data.find('corpora/opinion_lexicon')\n",
        "\n"
      ],
      "metadata": {
        "id": "QhVC5N3cQ71Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the positive and negative word lists from the uploaded files with explicit encoding\n",
        "positive_words = set([word.strip() for word in open('/content/positive-words.txt', encoding='ISO-8859-1')])\n",
        "negative_words = set([word.strip() for word in open('/content/negative-words.txt', encoding='ISO-8859-1')])\n",
        "\n",
        "# Print a sample of the words\n",
        "print(\"Sample of positive words:\", list(positive_words)[:5])\n",
        "print(\"Sample of negative words:\", list(negative_words)[:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "Hr4f1tHTUWyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verbs = ['destroyed', 'stolen', 'written', 'approved']\n",
        "\n",
        "for verb in verbs:\n",
        "    print(f\"Verb '{verb}' is positive: {verb in positive_words}\")\n",
        "    print(f\"Verb '{verb}' is negative: {verb in negative_words}\")\n"
      ],
      "metadata": {
        "id": "06H41uc6VX39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually add key verbs to the lexicon if necessary\n",
        "positive_words.update(['approved', 'written', 'handled'])\n",
        "negative_words.update(['destroyed', 'stolen'])\n",
        "\n",
        "# Re-run the analysis\n"
      ],
      "metadata": {
        "id": "ylAVPhX_W8nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to identify passive constructions and extract the past participle verbs\n",
        "def extract_passive_verbs(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    verbs = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Identify passive constructions via the dependency labels 'auxpass' and 'nsubjpass'\n",
        "        if token.dep_ == 'auxpass':  # Passive auxiliary (e.g., \"was\", \"is\", \"get\")\n",
        "            # Look for the main verb which should be a past participle in a passive\n",
        "            for child in token.head.children:\n",
        "                if child.dep_ == 'acl' or child.pos_ == 'VERB':  # Extract past participle verb\n",
        "                    verbs.append(child.lemma_)  # Use lemma to normalize the verb\n",
        "\n",
        "    return verbs\n",
        "\n",
        "# Function to analyze the sentiment of the verb tokens extracted from passive constructions\n",
        "def analyze_verb_sentiment(verbs, positive_words, negative_words):\n",
        "    sentiment_scores = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "\n",
        "    for verb in verbs:\n",
        "        if verb in positive_words:\n",
        "            sentiment_scores['positive'] += 1  # Count each token occurrence\n",
        "        elif verb in negative_words:\n",
        "            sentiment_scores['negative'] += 1\n",
        "        else:\n",
        "            sentiment_scores['neutral'] += 1\n",
        "\n",
        "    return sentiment_scores\n",
        "\n",
        "# Function to process each list of sentences (token-based approach)\n",
        "def analyze_sentences(sentences, positive_words, negative_words):\n",
        "    all_verbs = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        verbs = extract_passive_verbs(sentence)\n",
        "        all_verbs.extend(verbs)  # Collect all verbs across sentences\n",
        "\n",
        "    return analyze_verb_sentiment(all_verbs, positive_words, negative_words)\n",
        "\n",
        "\n",
        "# Perform sentiment analysis for each list\n",
        "bepp_spoken_sentiment = analyze_sentences(bepp_spoken, positive_words, negative_words)\n",
        "bepp_written_sentiment = analyze_sentences(bepp_written, positive_words, negative_words)\n",
        "getpp_spoken_sentiment = analyze_sentences(getpp_spoken, positive_words, negative_words)\n",
        "getpp_written_sentiment = analyze_sentences(getpp_written, positive_words, negative_words)\n",
        "\n",
        "# Display the results\n",
        "print(\"BePP Spoken Sentiment:\", bepp_spoken_sentiment)\n",
        "print(\"BePP Written Sentiment:\", bepp_written_sentiment)\n",
        "print(\"GetPP Spoken Sentiment:\", getpp_spoken_sentiment)\n",
        "print(\"GetPP Written Sentiment:\", getpp_written_sentiment)\n"
      ],
      "metadata": {
        "id": "1I_gFYvsN2Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated codes using vaderSentiment (word lists)\n",
        "\n",
        "+ If you want to automatically handle a wider range of verbs without manually modifying the lexicon, you can use a pre-built sentiment analysis tool like TextBlob or VADER. These models are more comprehensive and can classify sentiment for a broader range of words.\n",
        "\n",
        "+ Hutto, C. J., & Gilbert, E. E. (2014). VADER: A parsimonious rule-based model for sentiment analysis of social media text. Proceedings of the International AAAI Conference on Weblogs and Social Media, 216â€“225."
      ],
      "metadata": {
        "id": "O1SLhZ-QX0ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n"
      ],
      "metadata": {
        "id": "cbBiSZHLXoVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to analyze sentiment using VADER\n",
        "def analyze_sentences_vader(sentences):\n",
        "    sentiment_scores = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        score = analyzer.polarity_scores(sentence)\n",
        "        if score['compound'] >= 0.05:\n",
        "            sentiment_scores['positive'] += 1\n",
        "        elif score['compound'] <= -0.05:\n",
        "            sentiment_scores['negative'] += 1\n",
        "        else:\n",
        "            sentiment_scores['neutral'] += 1\n",
        "\n",
        "    return sentiment_scores\n",
        "\n",
        "# Test with dummy sentences\n",
        "bepp_spoken_sentiment = analyze_sentences_vader(bepp_spoken)\n",
        "bepp_written_sentiment = analyze_sentences_vader(bepp_written)\n",
        "getpp_spoken_sentiment = analyze_sentences_vader(getpp_spoken)\n",
        "getpp_written_sentiment = analyze_sentences_vader(getpp_written)\n",
        "\n",
        "print(\"BePP Spoken Sentiment (VADER):\", bepp_spoken_sentiment)\n",
        "print(\"BePP Written Sentiment (VADER):\", bepp_written_sentiment)\n",
        "print(\"GetPP Spoken Sentiment (VADER):\", getpp_spoken_sentiment)\n",
        "print(\"GetPP Written Sentiment (VADER):\", getpp_written_sentiment)\n"
      ],
      "metadata": {
        "id": "oqDfEuWJXyyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot: sentiment results"
      ],
      "metadata": {
        "id": "yHqMpKqpZHJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data from the results\n",
        "categories = ['BePP Spoken', 'BePP Written', 'GetPP Spoken', 'GetPP Written']\n",
        "positive = [3542, 5020, 120, 54]\n",
        "negative = [2579, 4011, 118, 34]\n",
        "neutral = [3426, 3659, 133, 44]\n",
        "\n",
        "# Create a bar plot for the sentiments\n",
        "def plot_sentiment_bar():\n",
        "    bar_width = 0.2\n",
        "    index = np.arange(len(categories))\n",
        "\n",
        "    # Create the bar chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(index, positive, bar_width, label='Positive', color='g')\n",
        "    plt.bar(index + bar_width, negative, bar_width, label='Negative', color='r')\n",
        "    plt.bar(index + 2 * bar_width, neutral, bar_width, label='Neutral', color='b')\n",
        "\n",
        "    # Add labels and formatting\n",
        "    plt.xlabel('Categories')\n",
        "    plt.ylabel('Counts')\n",
        "    plt.title('Sentiment Analysis for BePP and GetPP (Spoken vs Written)')\n",
        "    plt.xticks(index + bar_width, categories)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create a stacked bar plot for the sentiments\n",
        "def plot_sentiment_stacked_bar():\n",
        "    bar_width = 0.5\n",
        "    index = np.arange(len(categories))\n",
        "\n",
        "    # Create the stacked bar chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(index, positive, bar_width, label='Positive', color='g')\n",
        "    plt.bar(index, negative, bar_width, bottom=positive, label='Negative', color='r')\n",
        "    plt.bar(index, neutral, bar_width, bottom=np.array(positive) + np.array(negative), label='Neutral', color='b')\n",
        "\n",
        "    # Add labels and formatting\n",
        "    plt.xlabel('Categories')\n",
        "    plt.ylabel('Counts')\n",
        "    plt.title('Stacked Sentiment Analysis for BePP and GetPP (Spoken vs Written)')\n",
        "    plt.xticks(index, categories)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the functions to generate the plots\n",
        "plot_sentiment_bar()\n",
        "plot_sentiment_stacked_bar()\n"
      ],
      "metadata": {
        "id": "U6TxWuz_ZIhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II. Sentiment analysis with past participles (maybe this is a better way)"
      ],
      "metadata": {
        "id": "AjaHo_8UaavU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] data to read and get 4 sets of lists"
      ],
      "metadata": {
        "id": "wNVLiL3ca0ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Getpp24/main/data/resultall0815-light.csv\"\n",
        "\n",
        "# Step 1: Read the dataframe from the provided URL\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "\n",
        "# Function to check and convert strings that look like lists into actual lists\n",
        "def convert_to_list_if_needed(cell):\n",
        "    if isinstance(cell, str):\n",
        "        try:\n",
        "            # Try to convert it to a list\n",
        "            return ast.literal_eval(cell)\n",
        "        except (ValueError, SyntaxError):\n",
        "            # If it fails, return the original string\n",
        "            return cell\n",
        "    return cell\n",
        "\n",
        "# Apply the function to both 'Bepp' and 'Getpp' columns\n",
        "df['Bepp'] = df['Bepp'].apply(convert_to_list_if_needed)\n",
        "df['Getpp'] = df['Getpp'].apply(convert_to_list_if_needed)\n",
        "\n",
        "# Check the datatype of each cell in 'Bepp' and 'Getpp' and display row numbers if the cell is not a list\n",
        "non_list_bepp_rows = df[~df['Bepp'].apply(lambda x: isinstance(x, list))].index\n",
        "non_list_getpp_rows = df[~df['Getpp'].apply(lambda x: isinstance(x, list))].index\n",
        "\n",
        "# Output the row numbers where 'Bepp' is not a list\n",
        "if not non_list_bepp_rows.empty:\n",
        "    print(\"Rows where 'Bepp' is not a list:\")\n",
        "    print(non_list_bepp_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Bepp' column are lists.\")\n",
        "\n",
        "# Output the row numbers where 'Getpp' is not a list\n",
        "if not non_list_getpp_rows.empty:\n",
        "    print(\"Rows where 'Getpp' is not a list:\")\n",
        "    print(non_list_getpp_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Getpp' column are lists.\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Check DataFrame\n",
        "# print(\"Original DataFrame:\")\n",
        "# print(df)\n",
        "\n",
        "# Filter and combine sentence lists based on 'register'\n",
        "bepp_spoken = sum(df[df['register'] == 'Spoken']['Bepp'], [])\n",
        "bepp_written = sum(df[df['register'] == 'Written']['Bepp'], [])\n",
        "\n",
        "getpp_spoken = sum(df[df['register'] == 'Spoken']['Getpp'], [])\n",
        "getpp_written = sum(df[df['register'] == 'Written']['Getpp'], [])\n",
        "# Display the combined lists\n",
        "print(\"\\nCombined Sentences for Spoken Register: Be pp\")\n",
        "print(len(bepp_spoken), bepp_spoken)\n",
        "print(\"\\nCombined Sentences for Written Register: Be pp\")\n",
        "print(len(bepp_written), bepp_written)\n",
        "\n",
        "print(\"\\nCombined Sentences for Spoken Register: Get pp\")\n",
        "print(len(getpp_spoken), getpp_spoken)\n",
        "print(\"\\nCombined Sentences for Written Register: Get pp\")\n",
        "print(len(getpp_written), getpp_written)\n"
      ],
      "metadata": {
        "id": "Zq378ZbEagsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with 'Sentences' as the column name\n",
        "df_bepp_spoken = pd.DataFrame(bepp_spoken, columns=['Sentences'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_bepp_spoken.to_csv('bepp_spoken_sentences.csv', index=False)\n",
        "\n",
        "# Output the first few rows of the DataFrame as a check\n",
        "print(df_bepp_spoken.head())\n"
      ],
      "metadata": {
        "id": "ZX4dw8UZozOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Restart from here"
      ],
      "metadata": {
        "id": "KV3JXnPzq_dM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code processes:\n",
        "\n",
        "### Explanation:\n",
        "+ Lemmatization:\n",
        "We use spaCy's lemmatization (with token.lemma_) to handle all forms of \"be\" and \"get\" in different tenses and aspects. This also covers contractions, as spaCy normalizes forms like \"he's\" to \"be\" and \"he's gotten\" to \"get\".\n",
        "+ Past Participle Identification:\n",
        "We identify the past participle (VBN) that is dependent on the auxiliary verb (\"be\" or \"get\"). This ensures that we correctly extract the verb used in the passive construction.\n",
        "+ Handling All Forms of \"Be\" and \"Get\":\n",
        "The function handles all variations of \"be\" (e.g., \"is\", \"was\", \"were\", \"been\", \"being\") and \"get\" (e.g., \"got\", \"getting\"). These forms are normalized to \"be\" and \"get\" using lemmatization."
      ],
      "metadata": {
        "id": "zkkHo1IHce-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "BPzvl5oSe33d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract trigrams for 'be' passives using dependency parsing\n",
        "def extract_be_passive_trigrams(sentences):\n",
        "    trigrams = []\n",
        "\n",
        "    # Lemma forms of 'be' to match any tense or form\n",
        "    be_forms = {'be', 'is', 'am', 'are', 'was', 'were', 'been', 'being'}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        for token in doc:\n",
        "            # Check if the token is a form of 'be' and is an auxiliary verb\n",
        "            if token.lemma_ in be_forms and token.dep_ == 'aux':\n",
        "                head = token.head  # The head verb the auxiliary is attached to\n",
        "                if head.tag_ == 'VBN':  # Past participle form\n",
        "                    # Filter out irrelevant tokens (punctuation, subjects, etc.)\n",
        "                    children = [child.text for child in head.children if child.dep_ in {'advmod', 'aux', 'neg'}]\n",
        "                    trigram = (token.text, ' '.join(children), head.text)\n",
        "                    trigrams.append(trigram)\n",
        "\n",
        "    return trigrams\n",
        "\n",
        "# Function to extract trigrams for 'get' passives using dependency parsing\n",
        "def extract_get_passive_trigrams(sentences):\n",
        "    trigrams = []\n",
        "\n",
        "    # Lemma forms of 'get' to match any tense or form\n",
        "    get_forms = {'get', 'got', 'getting'}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        for token in doc:\n",
        "            # Check if the token is a form of 'get' and is an auxiliary verb\n",
        "            if token.lemma_ in get_forms and token.dep_ == 'aux':\n",
        "                head = token.head  # The head verb the auxiliary is attached to\n",
        "                if head.tag_ == 'VBN':  # Past participle form\n",
        "                    # Filter out irrelevant tokens (punctuation, subjects, etc.)\n",
        "                    children = [child.text for child in head.children if child.dep_ in {'advmod', 'aux', 'neg'}]\n",
        "                    trigram = (token.text, ' '.join(children), head.text)\n",
        "                    trigrams.append(trigram)\n",
        "\n",
        "    return trigrams\n",
        "\n",
        "\n",
        "# Example usage with separate datasets\n",
        "bepp_spoken1 = [\n",
        "    \"The building was completely destroyed.\",\n",
        "    \"The car had been quickly stolen.\",\n",
        "    \"They are being carefully arrested.\"\n",
        "]\n",
        "\n",
        "bepp_written1 = [\n",
        "    \"The report has been written by the committee.\",\n",
        "    \"The document was approved.\"\n",
        "]\n",
        "\n",
        "getpp_spoken1 = [\n",
        "    \"He got promoted last year.\",\n",
        "    \"The award is getting given to John.\",\n",
        "    \"The problem got solved quickly.\"\n",
        "]\n",
        "\n",
        "getpp_written1 = [\n",
        "    \"The issue will get handled by the team.\",\n",
        "    \"The task got completed.\"\n",
        "]\n",
        "\n",
        "# Extract trigrams for 'be' passives\n",
        "bepp_spoken_trigrams = extract_be_passive_trigrams(bepp_spoken1)\n",
        "bepp_written_trigrams = extract_be_passive_trigrams(bepp_written1)\n",
        "\n",
        "# Extract trigrams for 'get' passives\n",
        "getpp_spoken_trigrams = extract_get_passive_trigrams(getpp_spoken1)\n",
        "getpp_written_trigrams = extract_get_passive_trigrams(getpp_written1)\n",
        "\n",
        "# Output the extracted trigrams\n",
        "print(\"BePP Spoken Trigrams:\", bepp_spoken_trigrams)\n",
        "print(\"BePP Written Trigrams:\", bepp_written_trigrams)\n",
        "print(\"GetPP Spoken Trigrams:\", getpp_spoken_trigrams)\n",
        "print(\"GetPP Written Trigrams:\", getpp_written_trigrams)\n"
      ],
      "metadata": {
        "id": "xUcqg1vIj1J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract trigrams where the first word is a form of 'be'\n",
        "def extract_passive_trigrams(sentences):\n",
        "    trigrams = []\n",
        "\n",
        "    # Lemma forms of 'be' and 'get' to match any tense or form\n",
        "    be_forms = {'be', 'is', 'am', 'are', 'was', 'were', 'been', 'being', 'get', 'got', 'getting'}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        tokens = [token for token in doc]  # Tokenize the sentence\n",
        "\n",
        "        # Use a sliding window to extract trigrams (3-word sequences)\n",
        "        for i in range(len(tokens) - 2):\n",
        "            token1, token2, token3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "\n",
        "            # Check if the first token is a form of 'be' or 'get', and the third token is a past participle\n",
        "            if token1.lemma_ in be_forms and token3.tag_ == 'VBN':\n",
        "                trigram = (token1.text, token2.text, token3.text)\n",
        "                trigrams.append(trigram)\n",
        "\n",
        "    return trigrams\n",
        "\n",
        "\n",
        "# Extract trigrams for each dataset\n",
        "bepp_spoken_trigrams = extract_passive_trigrams(bepp_spoken)\n",
        "bepp_written_trigrams = extract_passive_trigrams(bepp_written)\n",
        "getpp_spoken_trigrams = extract_passive_trigrams(getpp_spoken)\n",
        "getpp_written_trigrams = extract_passive_trigrams(getpp_written)\n",
        "\n",
        "# Output the extracted trigrams\n",
        "print(\"BePP Spoken Trigrams:\", bepp_spoken_trigrams)\n",
        "print(\"BePP Written Trigrams:\", bepp_written_trigrams)\n",
        "print(\"GetPP Spoken Trigrams:\", getpp_spoken_trigrams)\n",
        "print(\"GetPP Written Trigrams:\", getpp_written_trigrams)\n"
      ],
      "metadata": {
        "id": "0cTks1Gnb0Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the output as dataframe"
      ],
      "metadata": {
        "id": "GJdhUtEwg9PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert list of tuples to a DataFrame\n",
        "df_bepp_spoken = pd.DataFrame(bepp_spoken_trigrams, columns=['1st', '2nd', '3rd'])\n",
        "df_bepp_written = pd.DataFrame(bepp_spoken_trigrams, columns=['1st', '2nd', '3rd'])\n",
        "df_getpp_spoken = pd.DataFrame(getpp_spoken_trigrams, columns=['1st', '2nd', '3rd'])\n",
        "df_getpp_written = pd.DataFrame(getpp_spoken_trigrams, columns=['1st', '2nd', '3rd'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_bepp_spoken)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_bepp_spoken.to_csv('bepp_spoken_trigrams.csv', index=False)\n",
        "df_bepp_written.to_csv('bepp_written_trigrams.csv', index=False)\n",
        "df_getpp_spoken.to_csv('getpp_spoken_trigrams.csv', index=False)\n",
        "df_getpp_written.to_csv('getpp_written_trigrams.csv', index=False)"
      ],
      "metadata": {
        "id": "8pGWCvGRgUYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis with polarity"
      ],
      "metadata": {
        "id": "jLTmKaEEXaBw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEScaVI5XdST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract past participles following any form of 'be' or 'get'\n",
        "def extract_past_participles(sentences, auxiliary_lemmas):\n",
        "    past_participles = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        for token in doc:\n",
        "            # Check if the token's lemma is 'be' or 'get' (covers all tenses/aspects including contractions)\n",
        "            if token.lemma_ in auxiliary_lemmas and token.dep_ == 'aux':\n",
        "                # Find the main verb (past participle) that depends on the auxiliary\n",
        "                for child in token.head.children:\n",
        "                    if child.pos_ == 'VERB' and child.tag_ == 'VBN':  # Past participle form\n",
        "                        past_participles.append(child.lemma_)  # Use lemma to normalize the verb\n",
        "\n",
        "    return past_participles\n",
        "\n",
        "\n",
        "# Extract past participles after 'be' and 'get'\n",
        "bepp_spoken = extract_past_participles(bepp_spoken, ['be'])\n",
        "bepp_written = extract_past_participles(bepp_written, ['be'])\n",
        "getpp_spoken = extract_past_participles(getpp_spoken, ['get'])\n",
        "getpp_written = extract_past_participles(getpp_written, ['get'])\n",
        "\n",
        "# Output the extracted past participles\n",
        "print(\"BePP Spoken Past Participles:\", bepp_spoken)\n",
        "print(\"BePP Written Past Participles:\", bepp_written)\n",
        "print(\"GetPP Spoken Past Participles:\", getpp_spoken)\n",
        "print(\"GetPP Written Past Participles:\", getpp_written)\n"
      ],
      "metadata": {
        "id": "KUYIBegea7Ak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}