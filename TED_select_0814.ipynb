{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNlCY9xJP4XLJ+nKV9KO4FQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Getpp24/blob/main/TED_select_0814.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TED process-data split to get 1.7 MW sized data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kx5ap3t1qUd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. With file 'selected0812', sample odd numbered rows and save it as 'selected-half.csv' => 3760 files to 1880 files half sized)"
      ],
      "metadata": {
        "id": "WfX4gkxprJEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1/2 size"
      ],
      "metadata": {
        "id": "XTGzyo9NuUkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('selected0812.csv', encoding='utf-8')\n",
        "\n",
        "# Select odd-numbered rows (remembering that Python uses 0-based indexing)\n",
        "df_odd_rows = df.iloc[::2]\n",
        "\n",
        "# Save the new DataFrame to a CSV file with UTF-8 encoding\n",
        "df_odd_rows.to_csv('selected-half.csv', index=False, encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "IJ2oEL-qqnzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1/3 of the size"
      ],
      "metadata": {
        "id": "P53L2JOhuScq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('selected0812.csv', encoding='utf-8')\n",
        "\n",
        "# Select every third row\n",
        "df_one_third = df.iloc[::3]\n",
        "\n",
        "# Save the new DataFrame to a CSV file with UTF-8 encoding\n",
        "df_one_third.to_csv('selected-one-third.csv', index=False, encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "mPKWWLXXuRpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One fourth size"
      ],
      "metadata": {
        "id": "xIjrKlb0zccS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('selected0812.csv', encoding='utf-8')\n",
        "\n",
        "# Select every third row\n",
        "df_one_third = df.iloc[::4]\n",
        "\n",
        "# Save the new DataFrame to a CSV file with UTF-8 encoding\n",
        "df_one_third.to_csv('selected-quarter.csv', index=False, encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "-RgDReKmvRFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Randome sample 1000 files"
      ],
      "metadata": {
        "id": "P4vrnEmWzhHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('selected0812.csv', encoding='utf-8')\n",
        "\n",
        "# Randomly select 1,000 rows\n",
        "df_random_1000 = df.sample(n=1000, random_state=42)  # random_state ensures reproducibility\n",
        "\n",
        "# Save the new DataFrame to a CSV file with UTF-8 encoding\n",
        "df_random_1000.to_csv('TED1000.csv', index=False, encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "KPd-y3U1zjmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Remove parenthesized texts and recover contraction forms for word count"
      ],
      "metadata": {
        "id": "hBr_kvIArFga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "\n",
        "# Make sure NLTK's punkt tokenizer models are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 1) Read the data with encoding 'utf-8'\n",
        "file_path = '/content/TED1000.csv'  # Update this path if needed\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "\n",
        "# 2) Remove any string within () or [] and save in a new column 'text-only'\n",
        "df['text-only'] = df['transcript'].apply(lambda x: re.sub(r'\\(.*?\\)|\\[.*?\\]', ' ', x))\n",
        "\n",
        "# 3) For each text in 'text-only', split 'nt, 've, 're and count words, record in 'Nword'\n",
        "def clean_and_count_words(text):\n",
        "    # Handle contractions\n",
        "    text = re.sub(r\"(?<=\\w)'t\", \" 't\", text)  # handle 'nt\n",
        "    text = re.sub(r\"(?<=\\w)'ve\", \" 've\", text)  # handle 've\n",
        "    text = re.sub(r\"(?<=\\w)'re\", \" 're\", text)  # handle 're\n",
        "    text = re.sub(r\"(?<=\\w)'d\", \" 'd\", text)  # handle 'd\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Filter out tokens that consist only of punctuation\n",
        "    words = [word for word in words if word not in string.punctuation]\n",
        "\n",
        "    return len(words)\n",
        "\n",
        "# Apply the function to the 'text-only' column\n",
        "df['Nword'] = df['text-only'].apply(clean_and_count_words)\n",
        "\n",
        "# 4) Count sentences and record in 'Nsent'\n",
        "df['Nsent'] = df['text-only'].apply(lambda x: len(sent_tokenize(x)))\n",
        "\n",
        "# 5) Calculate the total number of words and sentences\n",
        "total_words = df['Nword'].sum()\n",
        "total_sentences = df['Nsent'].sum()\n",
        "\n",
        "# Display the totals\n",
        "print(f'Total number of words: {total_words}')\n",
        "print(f'Total number of sentences: {total_sentences}')\n",
        "\n",
        "df.to_csv('/content/TED1000-processed.csv', encoding='utf-8', index=False)\n",
        "# Optionally, display the first few rows of the dataframe to verify\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "89ydgAHZs6Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v90dSNrddXgP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/selected-half.csv')\n",
        "\n",
        "# Display descriptive statistics for 'Nword' and 'Nsent' columns\n",
        "descriptive_stats = df[['Nword', 'Nsent']].describe()\n",
        "print(descriptive_stats)\n"
      ]
    }
  ]
}