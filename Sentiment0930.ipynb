{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPkrDmkA2yWtoez806z4fC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Getpp24/blob/main/Sentiment0930.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ± GetPP-Sentiment analysis (0821 9PM)"
      ],
      "metadata": {
        "id": "G5u3q_Kh6mdQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWLpFaFzIEgP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Getpp24/main/data/resultall0815-light.csv\"\n",
        "\n",
        "# Step 1: Read the dataframe from the provided URL\n",
        "df = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "lvj-HbXaINc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check cells of 'Bepp' and 'Getpp' whether they are in lists"
      ],
      "metadata": {
        "id": "kE8Wm4UbNfI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check the datatype of each cell in 'Bepp' and display row numbers if the cell is not a list\n",
        "non_list_rows = df[~df['Bepp'].apply(lambda x: isinstance(x, list))].index\n",
        "\n",
        "# Output the row numbers where 'Bepp' is not a list\n",
        "if not non_list_rows.empty:\n",
        "    print(\"Rows where 'Bepp' is not a list:\")\n",
        "    print(non_list_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Bepp' column are lists.\")\n"
      ],
      "metadata": {
        "id": "nAzxcwyIKz2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 'Bepp' cell as lists"
      ],
      "metadata": {
        "id": "iV2-WqwrLb3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "\n",
        "# Function to check and convert strings that look like lists into actual lists\n",
        "def convert_to_list_if_needed(cell):\n",
        "    if isinstance(cell, str):\n",
        "        try:\n",
        "            # Try to convert it to a list\n",
        "            return ast.literal_eval(cell)\n",
        "        except (ValueError, SyntaxError):\n",
        "            # If it fails, return the original string\n",
        "            return cell\n",
        "    return cell\n",
        "\n",
        "# Apply the function to both 'Bepp' and 'Getpp' columns\n",
        "df['Bepp'] = df['Bepp'].apply(convert_to_list_if_needed)\n",
        "df['Getpp'] = df['Getpp'].apply(convert_to_list_if_needed)\n",
        "\n",
        "# Check the datatype of each cell in 'Bepp' and 'Getpp' and display row numbers if the cell is not a list\n",
        "non_list_bepp_rows = df[~df['Bepp'].apply(lambda x: isinstance(x, list))].index\n",
        "non_list_getpp_rows = df[~df['Getpp'].apply(lambda x: isinstance(x, list))].index\n",
        "\n",
        "# Output the row numbers where 'Bepp' is not a list\n",
        "if not non_list_bepp_rows.empty:\n",
        "    print(\"Rows where 'Bepp' is not a list:\")\n",
        "    print(non_list_bepp_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Bepp' column are lists.\")\n",
        "\n",
        "# Output the row numbers where 'Getpp' is not a list\n",
        "if not non_list_getpp_rows.empty:\n",
        "    print(\"Rows where 'Getpp' is not a list:\")\n",
        "    print(non_list_getpp_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Getpp' column are lists.\")\n"
      ],
      "metadata": {
        "id": "a3R8nLsRLOi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bepp-spoken vs. Bepp-written: two lists of sentences"
      ],
      "metadata": {
        "id": "YKm4hiV9JF2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check DataFrame\n",
        "# print(\"Original DataFrame:\")\n",
        "# print(df)\n",
        "\n",
        "# Filter and combine sentence lists based on 'register'\n",
        "bepp_spoken = sum(df[df['register'] == 'Spoken']['Bepp'], [])\n",
        "bepp_written = sum(df[df['register'] == 'Written']['Bepp'], [])\n",
        "\n",
        "getpp_spoken = sum(df[df['register'] == 'Spoken']['Getpp'], [])\n",
        "getpp_written = sum(df[df['register'] == 'Written']['Getpp'], [])\n",
        "# Display the combined lists\n",
        "print(\"\\nCombined Sentences for Spoken Register: Be pp\")\n",
        "print(len(bepp_spoken), bepp_spoken)\n",
        "print(\"\\nCombined Sentences for Written Register: Be pp\")\n",
        "print(len(bepp_written), bepp_written)\n",
        "\n",
        "print(\"\\nCombined Sentences for Spoken Register: Get pp\")\n",
        "print(len(getpp_spoken), getpp_spoken)\n",
        "print(\"\\nCombined Sentences for Written Register: Get pp\")\n",
        "print(len(getpp_written), getpp_written)\n"
      ],
      "metadata": {
        "id": "l15ZKPxAJLTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis with 4 lists\n",
        "\n",
        "+ bepp_spoken, _written\n",
        "+ getpp_spoken, _written"
      ],
      "metadata": {
        "id": "3HJ851-aNlq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "0aGcvNCCLXd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('opinion_lexicon')\n",
        "nltk.data.find('corpora/opinion_lexicon')\n",
        "\n"
      ],
      "metadata": {
        "id": "QhVC5N3cQ71Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the positive and negative word lists from the uploaded files with explicit encoding\n",
        "positive_words = set([word.strip() for word in open('/content/positive-words.txt', encoding='ISO-8859-1')])\n",
        "negative_words = set([word.strip() for word in open('/content/negative-words.txt', encoding='ISO-8859-1')])\n",
        "\n",
        "# Print a sample of the words\n",
        "print(\"Sample of positive words:\", list(positive_words)[:5])\n",
        "print(\"Sample of negative words:\", list(negative_words)[:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "Hr4f1tHTUWyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "verbs = ['destroyed', 'stolen', 'written', 'approved']\n",
        "\n",
        "for verb in verbs:\n",
        "    print(f\"Verb '{verb}' is positive: {verb in positive_words}\")\n",
        "    print(f\"Verb '{verb}' is negative: {verb in negative_words}\")\n"
      ],
      "metadata": {
        "id": "06H41uc6VX39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually add key verbs to the lexicon if necessary\n",
        "positive_words.update(['approved', 'written', 'handled'])\n",
        "negative_words.update(['destroyed', 'stolen'])\n",
        "\n",
        "# Re-run the analysis\n"
      ],
      "metadata": {
        "id": "ylAVPhX_W8nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to identify passive constructions and extract the past participle verbs\n",
        "def extract_passive_verbs(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    verbs = []\n",
        "\n",
        "    for token in doc:\n",
        "        # Identify passive constructions via the dependency labels 'auxpass' and 'nsubjpass'\n",
        "        if token.dep_ == 'auxpass':  # Passive auxiliary (e.g., \"was\", \"is\", \"get\")\n",
        "            # Look for the main verb which should be a past participle in a passive\n",
        "            for child in token.head.children:\n",
        "                if child.dep_ == 'acl' or child.pos_ == 'VERB':  # Extract past participle verb\n",
        "                    verbs.append(child.lemma_)  # Use lemma to normalize the verb\n",
        "\n",
        "    return verbs\n",
        "\n",
        "# Function to analyze the sentiment of the verb tokens extracted from passive constructions\n",
        "def analyze_verb_sentiment(verbs, positive_words, negative_words):\n",
        "    sentiment_scores = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "\n",
        "    for verb in verbs:\n",
        "        if verb in positive_words:\n",
        "            sentiment_scores['positive'] += 1  # Count each token occurrence\n",
        "        elif verb in negative_words:\n",
        "            sentiment_scores['negative'] += 1\n",
        "        else:\n",
        "            sentiment_scores['neutral'] += 1\n",
        "\n",
        "    return sentiment_scores\n",
        "\n",
        "# Function to process each list of sentences (token-based approach)\n",
        "def analyze_sentences(sentences, positive_words, negative_words):\n",
        "    all_verbs = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        verbs = extract_passive_verbs(sentence)\n",
        "        all_verbs.extend(verbs)  # Collect all verbs across sentences\n",
        "\n",
        "    return analyze_verb_sentiment(all_verbs, positive_words, negative_words)\n",
        "\n",
        "\n",
        "# Perform sentiment analysis for each list\n",
        "bepp_spoken_sentiment = analyze_sentences(bepp_spoken, positive_words, negative_words)\n",
        "bepp_written_sentiment = analyze_sentences(bepp_written, positive_words, negative_words)\n",
        "getpp_spoken_sentiment = analyze_sentences(getpp_spoken, positive_words, negative_words)\n",
        "getpp_written_sentiment = analyze_sentences(getpp_written, positive_words, negative_words)\n",
        "\n",
        "# Display the results\n",
        "print(\"BePP Spoken Sentiment:\", bepp_spoken_sentiment)\n",
        "print(\"BePP Written Sentiment:\", bepp_written_sentiment)\n",
        "print(\"GetPP Spoken Sentiment:\", getpp_spoken_sentiment)\n",
        "print(\"GetPP Written Sentiment:\", getpp_written_sentiment)\n"
      ],
      "metadata": {
        "id": "1I_gFYvsN2Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated codes using vaderSentiment (word lists)\n",
        "\n",
        "+ If you want to automatically handle a wider range of verbs without manually modifying the lexicon, you can use a pre-built sentiment analysis tool like TextBlob or VADER. These models are more comprehensive and can classify sentiment for a broader range of words.\n",
        "\n",
        "+ Hutto, C. J., & Gilbert, E. E. (2014). VADER: A parsimonious rule-based model for sentiment analysis of social media text. Proceedings of the International AAAI Conference on Weblogs and Social Media, 216â€“225."
      ],
      "metadata": {
        "id": "O1SLhZ-QX0ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n"
      ],
      "metadata": {
        "id": "cbBiSZHLXoVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to analyze sentiment using VADER\n",
        "def analyze_sentences_vader(sentences):\n",
        "    sentiment_scores = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        score = analyzer.polarity_scores(sentence)\n",
        "        if score['compound'] >= 0.05:\n",
        "            sentiment_scores['positive'] += 1\n",
        "        elif score['compound'] <= -0.05:\n",
        "            sentiment_scores['negative'] += 1\n",
        "        else:\n",
        "            sentiment_scores['neutral'] += 1\n",
        "\n",
        "    return sentiment_scores\n",
        "\n",
        "# Test with dummy sentences\n",
        "bepp_spoken_sentiment = analyze_sentences_vader(bepp_spoken)\n",
        "bepp_written_sentiment = analyze_sentences_vader(bepp_written)\n",
        "getpp_spoken_sentiment = analyze_sentences_vader(getpp_spoken)\n",
        "getpp_written_sentiment = analyze_sentences_vader(getpp_written)\n",
        "\n",
        "print(\"BePP Spoken Sentiment (VADER):\", bepp_spoken_sentiment)\n",
        "print(\"BePP Written Sentiment (VADER):\", bepp_written_sentiment)\n",
        "print(\"GetPP Spoken Sentiment (VADER):\", getpp_spoken_sentiment)\n",
        "print(\"GetPP Written Sentiment (VADER):\", getpp_written_sentiment)\n"
      ],
      "metadata": {
        "id": "oqDfEuWJXyyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot: sentiment results"
      ],
      "metadata": {
        "id": "yHqMpKqpZHJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data from the results\n",
        "categories = ['BePP Spoken', 'BePP Written', 'GetPP Spoken', 'GetPP Written']\n",
        "positive = [3542, 5020, 120, 54]\n",
        "negative = [2579, 4011, 118, 34]\n",
        "neutral = [3426, 3659, 133, 44]\n",
        "\n",
        "# Create a bar plot for the sentiments\n",
        "def plot_sentiment_bar():\n",
        "    bar_width = 0.2\n",
        "    index = np.arange(len(categories))\n",
        "\n",
        "    # Create the bar chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(index, positive, bar_width, label='Positive', color='g')\n",
        "    plt.bar(index + bar_width, negative, bar_width, label='Negative', color='r')\n",
        "    plt.bar(index + 2 * bar_width, neutral, bar_width, label='Neutral', color='b')\n",
        "\n",
        "    # Add labels and formatting\n",
        "    plt.xlabel('Categories')\n",
        "    plt.ylabel('Counts')\n",
        "    plt.title('Sentiment Analysis for BePP and GetPP (Spoken vs Written)')\n",
        "    plt.xticks(index + bar_width, categories)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create a stacked bar plot for the sentiments\n",
        "def plot_sentiment_stacked_bar():\n",
        "    bar_width = 0.5\n",
        "    index = np.arange(len(categories))\n",
        "\n",
        "    # Create the stacked bar chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(index, positive, bar_width, label='Positive', color='g')\n",
        "    plt.bar(index, negative, bar_width, bottom=positive, label='Negative', color='r')\n",
        "    plt.bar(index, neutral, bar_width, bottom=np.array(positive) + np.array(negative), label='Neutral', color='b')\n",
        "\n",
        "    # Add labels and formatting\n",
        "    plt.xlabel('Categories')\n",
        "    plt.ylabel('Counts')\n",
        "    plt.title('Stacked Sentiment Analysis for BePP and GetPP (Spoken vs Written)')\n",
        "    plt.xticks(index, categories)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the functions to generate the plots\n",
        "plot_sentiment_bar()\n",
        "plot_sentiment_stacked_bar()\n"
      ],
      "metadata": {
        "id": "U6TxWuz_ZIhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalized sentiment plot (0929)"
      ],
      "metadata": {
        "id": "2NOP1iZHQdR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data from the results\n",
        "categories = ['BePP Spoken', 'BePP Written', 'GetPP Spoken', 'GetPP Written']\n",
        "positive = np.array([3542, 5020, 120, 54])\n",
        "negative = np.array([2579, 4011, 118, 34])\n",
        "neutral = np.array([3426, 3659, 133, 44])\n",
        "\n",
        "# Calculate the total number of sentiments for each category\n",
        "total = positive + negative + neutral\n",
        "\n",
        "# Normalize the sentiment counts to get proportions\n",
        "positive_proportion = positive / total\n",
        "negative_proportion = negative / total\n",
        "neutral_proportion = neutral / total\n",
        "\n",
        "# Create a stacked bar plot for the proportional sentiments\n",
        "def plot_proportional_stacked_bar():\n",
        "    bar_width = 0.5\n",
        "    index = np.arange(len(categories))\n",
        "\n",
        "    # Create the stacked bar chart with proportions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(index, positive_proportion, bar_width, label='Positive', color='g')\n",
        "    plt.bar(index, negative_proportion, bar_width, bottom=positive_proportion, label='Negative', color='r')\n",
        "    plt.bar(index, neutral_proportion, bar_width, bottom=positive_proportion + negative_proportion, label='Neutral', color='b')\n",
        "\n",
        "    # Add labels and formatting\n",
        "    plt.xlabel('Categories')\n",
        "    plt.ylabel('Proportion')\n",
        "    plt.title('Proportional Sentiment Analysis for BePP and GetPP (Spoken vs Written)')\n",
        "    plt.xticks(index, categories)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to generate the plot\n",
        "plot_proportional_stacked_bar()\n"
      ],
      "metadata": {
        "id": "DNWWmpx7QbqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data from the results\n",
        "categories = ['BePP Spoken', 'BePP Written', 'GetPP Spoken', 'GetPP Written']\n",
        "positive = [3542, 5020, 120, 54]\n",
        "negative = [2579, 4011, 118, 34]\n",
        "neutral = [3426, 3659, 133, 44]\n",
        "\n",
        "# Calculate the total number of sentiments for each category\n",
        "total = np.array(positive) + np.array(negative) + np.array(neutral)\n",
        "\n",
        "# Normalize the sentiment counts to get proportions\n",
        "positive_proportion = np.array(positive) / total\n",
        "negative_proportion = np.array(negative) / total\n",
        "neutral_proportion = np.array(neutral) / total\n",
        "\n",
        "# Create a grouped bar plot\n",
        "def plot_grouped_bar():\n",
        "    bar_width = 0.25\n",
        "    index = np.arange(len(categories))\n",
        "\n",
        "    # Create the grouped bar chart\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(index, positive_proportion, bar_width, label='Positive', color='g')\n",
        "    plt.bar(index + bar_width, negative_proportion, bar_width, label='Negative', color='r')\n",
        "    plt.bar(index + 2 * bar_width, neutral_proportion, bar_width, label='Neutral', color='b')\n",
        "\n",
        "    # Add labels and formatting\n",
        "    plt.xlabel('Categories')\n",
        "    plt.ylabel('Proportion')\n",
        "    plt.title('Proportional Sentiment Analysis by Register and Construction Type')\n",
        "    plt.xticks(index + bar_width, categories)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to generate the grouped bar plot\n",
        "plot_grouped_bar()\n"
      ],
      "metadata": {
        "id": "HXRKogEwRYBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Heatmap"
      ],
      "metadata": {
        "id": "NgmWFa8RR4WE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data from the results\n",
        "categories = ['BePP Spoken', 'BePP Written', 'GetPP Spoken', 'GetPP Written']\n",
        "positive = [3542, 5020, 120, 54]\n",
        "negative = [2579, 4011, 118, 34]\n",
        "neutral = [3426, 3659, 133, 44]\n",
        "\n",
        "# Create a matrix of sentiment proportions\n",
        "sentiment_matrix = np.array([\n",
        "    [3542, 5020, 120, 54],  # Positive\n",
        "    [2579, 4011, 118, 34],  # Negative\n",
        "    [3426, 3659, 133, 44]   # Neutral\n",
        "])\n",
        "\n",
        "# Normalize the matrix to show proportions\n",
        "total = sentiment_matrix.sum(axis=0)\n",
        "sentiment_matrix_proportions = sentiment_matrix / total\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(sentiment_matrix_proportions, annot=True, cmap='coolwarm', xticklabels=categories, yticklabels=['Positive', 'Negative', 'Neutral'])\n",
        "plt.title('Sentiment Proportions by Register and Construction Type')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Sentiment Type')\n",
        "plt.savefig('Sentiment-heatmap.png')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZApJxhrtR07W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chi-squared test with sentiment results"
      ],
      "metadata": {
        "id": "6IZrfPxhWEci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "import numpy as np\n",
        "\n",
        "# Observed data (in contingency table format)\n",
        "observed_data = np.array([\n",
        "    [3542, 5020, 120, 54],  # Positive counts\n",
        "    [2579, 4011, 118, 34],  # Negative counts\n",
        "    [3426, 3659, 133, 44]   # Neutral counts\n",
        "])\n",
        "\n",
        "# Perform the chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(observed_data)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Chi-square statistic: {chi2}\")\n",
        "print(f\"P-value: {p}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(f\"Expected frequencies: \\n{expected}\")\n"
      ],
      "metadata": {
        "id": "-2bIqgRaWHq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residuals and contributions"
      ],
      "metadata": {
        "id": "bzxHj05CW9qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Data from the results\n",
        "observed = np.array([[3542, 5020, 120, 54],\n",
        "                     [2579, 4011, 118, 34],\n",
        "                     [3426, 3659, 133, 44]])\n",
        "\n",
        "# Perform Chi-square test\n",
        "chi2_stat, p_val, dof, expected = chi2_contingency(observed)\n",
        "\n",
        "# Calculate residuals and contributions\n",
        "residuals = (observed - expected) / np.sqrt(expected)\n",
        "contributions = (observed - expected)**2 / expected\n",
        "\n",
        "# Create a dataframe to display results\n",
        "categories = ['BePP Spoken', 'BePP Written', 'GetPP Spoken', 'GetPP Written']\n",
        "index = ['Positive', 'Negative', 'Neutral']\n",
        "\n",
        "# Convert arrays to DataFrames for better presentation\n",
        "observed_df = pd.DataFrame(observed, columns=categories, index=index)\n",
        "expected_df = pd.DataFrame(expected, columns=categories, index=index)\n",
        "residuals_df = pd.DataFrame(residuals, columns=categories, index=index)\n",
        "contributions_df = pd.DataFrame(contributions, columns=categories, index=index)\n",
        "\n",
        "# Display results\n",
        "print(\"Observed Frequencies:\\n\", observed_df)\n",
        "print(\"\\nExpected Frequencies:\\n\", expected_df)\n",
        "print(\"\\nResiduals:\\n\", residuals_df)\n",
        "print(\"\\nChi-square Contributions:\\n\", contributions_df)\n",
        "\n",
        "# Optional: visualize residuals as a heatmap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(residuals_df, annot=True, cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Residuals Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(contributions_df, annot=True, cmap=\"YlGnBu\")\n",
        "plt.title(\"Chi-square Contributions Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tmbol5K5XADT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II. Sentiment analysis with past participles (maybe this is a better way)"
      ],
      "metadata": {
        "id": "AjaHo_8UaavU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] data to read and get 4 sets of lists"
      ],
      "metadata": {
        "id": "wNVLiL3ca0ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/MK316/Getpp24/main/data/resultall0815-light.csv\"\n",
        "\n",
        "# Step 1: Read the dataframe from the provided URL\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "\n",
        "# Function to check and convert strings that look like lists into actual lists\n",
        "def convert_to_list_if_needed(cell):\n",
        "    if isinstance(cell, str):\n",
        "        try:\n",
        "            # Try to convert it to a list\n",
        "            return ast.literal_eval(cell)\n",
        "        except (ValueError, SyntaxError):\n",
        "            # If it fails, return the original string\n",
        "            return cell\n",
        "    return cell\n",
        "\n",
        "# Apply the function to both 'Bepp' and 'Getpp' columns\n",
        "df['Bepp'] = df['Bepp'].apply(convert_to_list_if_needed)\n",
        "df['Getpp'] = df['Getpp'].apply(convert_to_list_if_needed)\n",
        "\n",
        "# Check the datatype of each cell in 'Bepp' and 'Getpp' and display row numbers if the cell is not a list\n",
        "non_list_bepp_rows = df[~df['Bepp'].apply(lambda x: isinstance(x, list))].index\n",
        "non_list_getpp_rows = df[~df['Getpp'].apply(lambda x: isinstance(x, list))].index\n",
        "\n",
        "# Output the row numbers where 'Bepp' is not a list\n",
        "if not non_list_bepp_rows.empty:\n",
        "    print(\"Rows where 'Bepp' is not a list:\")\n",
        "    print(non_list_bepp_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Bepp' column are lists.\")\n",
        "\n",
        "# Output the row numbers where 'Getpp' is not a list\n",
        "if not non_list_getpp_rows.empty:\n",
        "    print(\"Rows where 'Getpp' is not a list:\")\n",
        "    print(non_list_getpp_rows.tolist())\n",
        "else:\n",
        "    print(\"All cells in the 'Getpp' column are lists.\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Check DataFrame\n",
        "# print(\"Original DataFrame:\")\n",
        "# print(df)\n",
        "\n",
        "# Filter and combine sentence lists based on 'register'\n",
        "bepp_spoken = sum(df[df['register'] == 'Spoken']['Bepp'], [])\n",
        "bepp_written = sum(df[df['register'] == 'Written']['Bepp'], [])\n",
        "\n",
        "getpp_spoken = sum(df[df['register'] == 'Spoken']['Getpp'], [])\n",
        "getpp_written = sum(df[df['register'] == 'Written']['Getpp'], [])\n",
        "# Display the combined lists\n",
        "print(\"\\nCombined Sentences for Spoken Register: Be pp\")\n",
        "print(len(bepp_spoken), bepp_spoken)\n",
        "print(\"\\nCombined Sentences for Written Register: Be pp\")\n",
        "print(len(bepp_written), bepp_written)\n",
        "\n",
        "print(\"\\nCombined Sentences for Spoken Register: Get pp\")\n",
        "print(len(getpp_spoken), getpp_spoken)\n",
        "print(\"\\nCombined Sentences for Written Register: Get pp\")\n",
        "print(len(getpp_written), getpp_written)\n"
      ],
      "metadata": {
        "id": "Zq378ZbEagsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Restart from here"
      ],
      "metadata": {
        "id": "KV3JXnPzq_dM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try quadrogram with bepp_spoken"
      ],
      "metadata": {
        "id": "02lQOj8htj6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] quadrogram be pp => csv"
      ],
      "metadata": {
        "id": "V5nygMC9uhjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "# Create a DataFrame\n",
        "df_bepp_spoken = pd.DataFrame(bepp_spoken, columns=['Sentences'])\n",
        "\n",
        "# Lemma forms of 'be' to match any tense or form including contractions\n",
        "be_forms = {'be', 'is', 'am', 'are', \"'s\", 'was', 'were', 'been', 'being'}\n",
        "\n",
        "# Function to extract quadrograms starting with any 'be' form verb\n",
        "def extract_quadrograms(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    quadrograms = []\n",
        "\n",
        "    # Loop over tokens and extract quadrograms starting with any 'be' verb\n",
        "    for i in range(len(doc) - 3):\n",
        "        token1, token2, token3, token4 = doc[i], doc[i+1], doc[i+2], doc[i+3]\n",
        "\n",
        "        # Check if the first token is a form of 'be'\n",
        "        if token1.lemma_ in be_forms:\n",
        "            quadrogram = (token1.text, token2.text, token3.text, token4.text)\n",
        "            quadrograms.append(quadrogram)\n",
        "\n",
        "    return quadrograms\n",
        "\n",
        "# List to hold all extracted quadrograms\n",
        "quadrogram_list = []\n",
        "\n",
        "# Extract quadrograms for each sentence in the DataFrame\n",
        "for sentence in df_bepp_spoken['Sentences']:\n",
        "    quadrograms = extract_quadrograms(sentence)\n",
        "    quadrogram_list.extend(quadrograms)\n",
        "\n",
        "# Create a DataFrame from the extracted quadrograms\n",
        "df_quadrograms = pd.DataFrame(quadrogram_list, columns=['1st', '2nd', '3rd', '4th'])\n",
        "\n",
        "# Save the DataFrame to a CSV file (optional)\n",
        "df_quadrograms.to_csv('quadrograms-bepp-spoken.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df_quadrograms.head())\n"
      ],
      "metadata": {
        "id": "ipUuYVRytocR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[2] quadrogram be_written => csv"
      ],
      "metadata": {
        "id": "NcCLzXqdunm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "# Create a DataFrame\n",
        "df_bepp_written = pd.DataFrame(bepp_written, columns=['Sentences'])\n",
        "\n",
        "# Lemma forms of 'be' to match any tense or form including contractions\n",
        "be_forms = {'be', 'is', 'am', 'are', \"'s\", 'was', 'were', 'been', 'being'}\n",
        "\n",
        "# Function to extract quadrograms starting with any 'be' form verb\n",
        "def extract_quadrograms(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    quadrograms = []\n",
        "\n",
        "    # Loop over tokens and extract quadrograms starting with any 'be' verb\n",
        "    for i in range(len(doc) - 3):\n",
        "        token1, token2, token3, token4 = doc[i], doc[i+1], doc[i+2], doc[i+3]\n",
        "\n",
        "        # Check if the first token is a form of 'be'\n",
        "        if token1.lemma_ in be_forms:\n",
        "            quadrogram = (token1.text, token2.text, token3.text, token4.text)\n",
        "            quadrograms.append(quadrogram)\n",
        "\n",
        "    return quadrograms\n",
        "\n",
        "# List to hold all extracted quadrograms\n",
        "quadrogram_list = []\n",
        "\n",
        "# Extract quadrograms for each sentence in the DataFrame\n",
        "for sentence in df_bepp_written['Sentences']:\n",
        "    quadrograms = extract_quadrograms(sentence)\n",
        "    quadrogram_list.extend(quadrograms)\n",
        "\n",
        "# Create a DataFrame from the extracted quadrograms\n",
        "df_quadrograms = pd.DataFrame(quadrogram_list, columns=['1st', '2nd', '3rd', '4th'])\n",
        "\n",
        "# Save the DataFrame to a CSV file (optional)\n",
        "df_quadrograms.to_csv('quadrograms-bepp-written.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df_quadrograms.head())\n"
      ],
      "metadata": {
        "id": "EOJJJFZzur0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[3] quadrogram get spoken"
      ],
      "metadata": {
        "id": "eHsZtPgkv4_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "# Create a DataFrame\n",
        "df_getpp_spoken = pd.DataFrame(getpp_spoken, columns=['Sentences'])\n",
        "\n",
        "# Lemma forms of 'be' to match any tense or form including contractions\n",
        "get_forms = {'get', 'got', 'getting'}\n",
        "\n",
        "# Function to extract quadrograms starting with any 'be' form verb\n",
        "def extract_quadrograms(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    quadrograms = []\n",
        "\n",
        "    # Loop over tokens and extract quadrograms starting with any 'be' verb\n",
        "    for i in range(len(doc) - 3):\n",
        "        token1, token2, token3, token4 = doc[i], doc[i+1], doc[i+2], doc[i+3]\n",
        "\n",
        "        # Check if the first token is a form of 'be'\n",
        "        if token1.lemma_ in get_forms:\n",
        "            quadrogram = (token1.text, token2.text, token3.text, token4.text)\n",
        "            quadrograms.append(quadrogram)\n",
        "\n",
        "    return quadrograms\n",
        "\n",
        "# List to hold all extracted quadrograms\n",
        "quadrogram_list = []\n",
        "\n",
        "# Extract quadrograms for each sentence in the DataFrame\n",
        "for sentence in df_getpp_spoken['Sentences']:\n",
        "    quadrograms = extract_quadrograms(sentence)\n",
        "    quadrogram_list.extend(quadrograms)\n",
        "\n",
        "# Create a DataFrame from the extracted quadrograms\n",
        "df_quadrograms = pd.DataFrame(quadrogram_list, columns=['1st', '2nd', '3rd', '4th'])\n",
        "\n",
        "# Save the DataFrame to a CSV file (optional)\n",
        "df_quadrograms.to_csv('quadrograms-getpp-spoken.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df_quadrograms.head())\n"
      ],
      "metadata": {
        "id": "wH3ub3UEv8Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4] quadrogram get written"
      ],
      "metadata": {
        "id": "K1rwn725v9Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "# Create a DataFrame\n",
        "df_getpp_written = pd.DataFrame(getpp_written, columns=['Sentences'])\n",
        "\n",
        "# Lemma forms of 'be' to match any tense or form including contractions\n",
        "get_forms = {'get', 'got', 'getting'}\n",
        "\n",
        "# Function to extract quadrograms starting with any 'be' form verb\n",
        "def extract_quadrograms(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    quadrograms = []\n",
        "\n",
        "    # Loop over tokens and extract quadrograms starting with any 'be' verb\n",
        "    for i in range(len(doc) - 3):\n",
        "        token1, token2, token3, token4 = doc[i], doc[i+1], doc[i+2], doc[i+3]\n",
        "\n",
        "        # Check if the first token is a form of 'be'\n",
        "        if token1.lemma_ in get_forms:\n",
        "            quadrogram = (token1.text, token2.text, token3.text, token4.text)\n",
        "            quadrograms.append(quadrogram)\n",
        "\n",
        "    return quadrograms\n",
        "\n",
        "# List to hold all extracted quadrograms\n",
        "quadrogram_list = []\n",
        "\n",
        "# Extract quadrograms for each sentence in the DataFrame\n",
        "for sentence in df_getpp_written['Sentences']:\n",
        "    quadrograms = extract_quadrograms(sentence)\n",
        "    quadrogram_list.extend(quadrograms)\n",
        "\n",
        "# Create a DataFrame from the extracted quadrograms\n",
        "df_quadrograms = pd.DataFrame(quadrogram_list, columns=['1st', '2nd', '3rd', '4th'])\n",
        "\n",
        "# Save the DataFrame to a CSV file (optional)\n",
        "df_quadrograms.to_csv('quadrograms-getpp-written.csv', index=False)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df_quadrograms.head())\n"
      ],
      "metadata": {
        "id": "8J_rU7CnwKaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To sentences (dataframe): 4 files saved\n",
        "\n",
        "+ no need to run below"
      ],
      "metadata": {
        "id": "6VsISNxPuszA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with 'Sentences' as the column name\n",
        "df_bepp_spoken = pd.DataFrame(bepp_spoken, columns=['Sentences'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_bepp_spoken.to_csv('bepp_spoken_sentences.csv', index=False)\n",
        "\n",
        "# Output the first few rows of the DataFrame as a check\n",
        "print(df_bepp_spoken.head())\n"
      ],
      "metadata": {
        "id": "ZX4dw8UZozOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code processes:\n",
        "\n",
        "### Explanation:\n",
        "+ Lemmatization:\n",
        "We use spaCy's lemmatization (with token.lemma_) to handle all forms of \"be\" and \"get\" in different tenses and aspects. This also covers contractions, as spaCy normalizes forms like \"he's\" to \"be\" and \"he's gotten\" to \"get\".\n",
        "+ Past Participle Identification:\n",
        "We identify the past participle (VBN) that is dependent on the auxiliary verb (\"be\" or \"get\"). This ensures that we correctly extract the verb used in the passive construction.\n",
        "+ Handling All Forms of \"Be\" and \"Get\":\n",
        "The function handles all variations of \"be\" (e.g., \"is\", \"was\", \"were\", \"been\", \"being\") and \"get\" (e.g., \"got\", \"getting\"). These forms are normalized to \"be\" and \"get\" using lemmatization."
      ],
      "metadata": {
        "id": "zkkHo1IHce-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "BPzvl5oSe33d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract trigrams for 'be' passives using dependency parsing\n",
        "def extract_be_passive_trigrams(sentences):\n",
        "    trigrams = []\n",
        "\n",
        "    # Lemma forms of 'be' to match any tense or form\n",
        "    be_forms = {'be', 'is', 'am', 'are', 'was', 'were', 'been', 'being'}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        for token in doc:\n",
        "            # Check if the token is a form of 'be' and is an auxiliary verb\n",
        "            if token.lemma_ in be_forms and token.dep_ == 'aux':\n",
        "                head = token.head  # The head verb the auxiliary is attached to\n",
        "                if head.tag_ == 'VBN':  # Past participle form\n",
        "                    # Filter out irrelevant tokens (punctuation, subjects, etc.)\n",
        "                    children = [child.text for child in head.children if child.dep_ in {'advmod', 'aux', 'neg'}]\n",
        "                    trigram = (token.text, ' '.join(children), head.text)\n",
        "                    trigrams.append(trigram)\n",
        "\n",
        "    return trigrams\n",
        "\n",
        "# Function to extract trigrams for 'get' passives using dependency parsing\n",
        "def extract_get_passive_trigrams(sentences):\n",
        "    trigrams = []\n",
        "\n",
        "    # Lemma forms of 'get' to match any tense or form\n",
        "    get_forms = {'get', 'got', 'getting'}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        for token in doc:\n",
        "            # Check if the token is a form of 'get' and is an auxiliary verb\n",
        "            if token.lemma_ in get_forms and token.dep_ == 'aux':\n",
        "                head = token.head  # The head verb the auxiliary is attached to\n",
        "                if head.tag_ == 'VBN':  # Past participle form\n",
        "                    # Filter out irrelevant tokens (punctuation, subjects, etc.)\n",
        "                    children = [child.text for child in head.children if child.dep_ in {'advmod', 'aux', 'neg'}]\n",
        "                    trigram = (token.text, ' '.join(children), head.text)\n",
        "                    trigrams.append(trigram)\n",
        "\n",
        "    return trigrams\n",
        "\n",
        "\n",
        "# Example usage with separate datasets\n",
        "bepp_spoken1 = [\n",
        "    \"The building was completely destroyed.\",\n",
        "    \"The car had been quickly stolen.\",\n",
        "    \"They are being carefully arrested.\"\n",
        "]\n",
        "\n",
        "bepp_written1 = [\n",
        "    \"The report has been written by the committee.\",\n",
        "    \"The document was approved.\"\n",
        "]\n",
        "\n",
        "getpp_spoken1 = [\n",
        "    \"He got promoted last year.\",\n",
        "    \"The award is getting given to John.\",\n",
        "    \"The problem got solved quickly.\"\n",
        "]\n",
        "\n",
        "getpp_written1 = [\n",
        "    \"The issue will get handled by the team.\",\n",
        "    \"The task got completed.\"\n",
        "]\n",
        "\n",
        "# Extract trigrams for 'be' passives\n",
        "bepp_spoken_trigrams = extract_be_passive_trigrams(bepp_spoken1)\n",
        "bepp_written_trigrams = extract_be_passive_trigrams(bepp_written1)\n",
        "\n",
        "# Extract trigrams for 'get' passives\n",
        "getpp_spoken_trigrams = extract_get_passive_trigrams(getpp_spoken1)\n",
        "getpp_written_trigrams = extract_get_passive_trigrams(getpp_written1)\n",
        "\n",
        "# Output the extracted trigrams\n",
        "print(\"BePP Spoken Trigrams:\", bepp_spoken_trigrams)\n",
        "print(\"BePP Written Trigrams:\", bepp_written_trigrams)\n",
        "print(\"GetPP Spoken Trigrams:\", getpp_spoken_trigrams)\n",
        "print(\"GetPP Written Trigrams:\", getpp_written_trigrams)\n"
      ],
      "metadata": {
        "id": "xUcqg1vIj1J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract trigrams where the first word is a form of 'be'\n",
        "def extract_passive_trigrams(sentences):\n",
        "    trigrams = []\n",
        "\n",
        "    # Lemma forms of 'be' and 'get' to match any tense or form\n",
        "    be_forms = {'be', 'is', 'am', 'are', 'was', 'were', 'been', 'being', 'get', 'got', 'getting'}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        tokens = [token for token in doc]  # Tokenize the sentence\n",
        "\n",
        "        # Use a sliding window to extract trigrams (3-word sequences)\n",
        "        for i in range(len(tokens) - 2):\n",
        "            token1, token2, token3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "\n",
        "            # Check if the first token is a form of 'be' or 'get', and the third token is a past participle\n",
        "            if token1.lemma_ in be_forms and token3.tag_ == 'VBN':\n",
        "                trigram = (token1.text, token2.text, token3.text)\n",
        "                trigrams.append(trigram)\n",
        "\n",
        "    return trigrams\n",
        "\n",
        "\n",
        "# Extract trigrams for each dataset\n",
        "bepp_spoken_trigrams = extract_passive_trigrams(bepp_spoken)\n",
        "bepp_written_trigrams = extract_passive_trigrams(bepp_written)\n",
        "getpp_spoken_trigrams = extract_passive_trigrams(getpp_spoken)\n",
        "getpp_written_trigrams = extract_passive_trigrams(getpp_written)\n",
        "\n",
        "# Output the extracted trigrams\n",
        "print(\"BePP Spoken Trigrams:\", bepp_spoken_trigrams)\n",
        "print(\"BePP Written Trigrams:\", bepp_written_trigrams)\n",
        "print(\"GetPP Spoken Trigrams:\", getpp_spoken_trigrams)\n",
        "print(\"GetPP Written Trigrams:\", getpp_written_trigrams)\n"
      ],
      "metadata": {
        "id": "0cTks1Gnb0Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the output as dataframe"
      ],
      "metadata": {
        "id": "GJdhUtEwg9PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert list of tuples to a DataFrame\n",
        "df_bepp_spoken = pd.DataFrame(bepp_spoken_trigrams, columns=['1st', '2nd', '3rd'])\n",
        "df_bepp_written = pd.DataFrame(bepp_spoken_trigrams, columns=['1st', '2nd', '3rd'])\n",
        "df_getpp_spoken = pd.DataFrame(getpp_spoken_trigrams, columns=['1st', '2nd', '3rd'])\n",
        "df_getpp_written = pd.DataFrame(getpp_spoken_trigrams, columns=['1st', '2nd', '3rd'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_bepp_spoken)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_bepp_spoken.to_csv('bepp_spoken_trigrams.csv', index=False)\n",
        "df_bepp_written.to_csv('bepp_written_trigrams.csv', index=False)\n",
        "df_getpp_spoken.to_csv('getpp_spoken_trigrams.csv', index=False)\n",
        "df_getpp_written.to_csv('getpp_written_trigrams.csv', index=False)"
      ],
      "metadata": {
        "id": "8pGWCvGRgUYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis with polarity"
      ],
      "metadata": {
        "id": "jLTmKaEEXaBw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEScaVI5XdST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract past participles following any form of 'be' or 'get'\n",
        "def extract_past_participles(sentences, auxiliary_lemmas):\n",
        "    past_participles = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        for token in doc:\n",
        "            # Check if the token's lemma is 'be' or 'get' (covers all tenses/aspects including contractions)\n",
        "            if token.lemma_ in auxiliary_lemmas and token.dep_ == 'aux':\n",
        "                # Find the main verb (past participle) that depends on the auxiliary\n",
        "                for child in token.head.children:\n",
        "                    if child.pos_ == 'VERB' and child.tag_ == 'VBN':  # Past participle form\n",
        "                        past_participles.append(child.lemma_)  # Use lemma to normalize the verb\n",
        "\n",
        "    return past_participles\n",
        "\n",
        "\n",
        "# Extract past participles after 'be' and 'get'\n",
        "bepp_spoken = extract_past_participles(bepp_spoken, ['be'])\n",
        "bepp_written = extract_past_participles(bepp_written, ['be'])\n",
        "getpp_spoken = extract_past_participles(getpp_spoken, ['get'])\n",
        "getpp_written = extract_past_participles(getpp_written, ['get'])\n",
        "\n",
        "# Output the extracted past participles\n",
        "print(\"BePP Spoken Past Participles:\", bepp_spoken)\n",
        "print(\"BePP Written Past Participles:\", bepp_written)\n",
        "print(\"GetPP Spoken Past Participles:\", getpp_spoken)\n",
        "print(\"GetPP Written Past Participles:\", getpp_written)\n"
      ],
      "metadata": {
        "id": "KUYIBegea7Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bepp_spoken)"
      ],
      "metadata": {
        "id": "jKlTx5cPbzZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of participles"
      ],
      "metadata": {
        "id": "ioOmo-y3cw6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract past participles following any form of 'be' or 'get'\n",
        "def extract_past_participles(sentences, auxiliary_lemmas):\n",
        "    past_participles = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(sentence)\n",
        "        for token in doc:\n",
        "            # Check if the token's lemma is 'be' or 'get'\n",
        "            if token.lemma_ in auxiliary_lemmas:\n",
        "                # Find the main verb (past participle) that depends on the auxiliary OR \"get\" as a main verb\n",
        "                for child in token.head.children:\n",
        "                    if child.pos_ == 'VERB' and child.tag_ == 'VBN':  # Past participle form\n",
        "                        past_participles.append(child.lemma_)  # Use lemma to normalize the verb\n",
        "\n",
        "                # Handle the case where \"get\" is the main verb (root of the sentence)\n",
        "                if token.lemma_ == 'get' and token.pos_ == 'VERB' and token.tag_ == 'VBN':\n",
        "                    past_participles.append(token.head.lemma_)\n",
        "\n",
        "    return past_participles\n",
        "\n",
        "\n",
        "# Extract past participles after 'be' and 'get'\n",
        "bepp_spoken = extract_past_participles(bepp_spoken, ['be'])\n",
        "bepp_written = extract_past_participles(bepp_written, ['be'])\n",
        "getpp_spoken = extract_past_participles(getpp_spoken, ['get'])\n",
        "getpp_written = extract_past_participles(getpp_written, ['get'])\n",
        "\n",
        "# Output the extracted past participles\n",
        "print(\"BePP Spoken Past Participles:\", bepp_spoken)\n",
        "print(\"BePP Written Past Participles:\", bepp_written)\n",
        "print(\"GetPP Spoken Past Participles:\", getpp_spoken)\n",
        "print(\"GetPP Written Past Participles:\", getpp_written)\n"
      ],
      "metadata": {
        "id": "LpAArx4KcvDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model for English\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Function to extract past participles for passive constructions\n",
        "def extract_past_participles(sentences, auxiliary_lemmas):\n",
        "    past_participles = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        print(f\"\\nProcessing sentence: {sentence}\")\n",
        "        doc = nlp(sentence)\n",
        "\n",
        "        # Print the token, lemma, pos, tag, and dependency relation for each token\n",
        "        for token in doc:\n",
        "            print(f\"Token: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}, Tag: {token.tag_}, Dep: {token.dep_}, Head: {token.head.text}\")\n",
        "\n",
        "        for token in doc:\n",
        "            # Check if the token's lemma is 'be' or 'get'\n",
        "            if token.lemma_ in auxiliary_lemmas:\n",
        "                # Check if the main verb (past participle) is the head or a child of the auxiliary verb\n",
        "                for child in token.head.children:\n",
        "                    if child.pos_ == 'VERB' and child.tag_ == 'VBN':  # Past participle form\n",
        "                        past_participles.append(child.text)  # Append the actual token text\n",
        "\n",
        "                # Handle case where \"get\" is the main verb followed by a past participle\n",
        "                if token.lemma_ == 'get' and token.dep_ == 'ROOT':  # Check if 'get' is the root\n",
        "                    for child in token.children:\n",
        "                        if child.pos_ == 'VERB' and child.tag_ == 'VBN':  # Past participle form\n",
        "                            past_participles.append(child.text)\n",
        "\n",
        "    return past_participles\n",
        "\n",
        "# Example data (replace these with your actual data)\n",
        "bepp_spoken = [\"The book was written by the author.\", \"The letter is being delivered.\"]\n",
        "bepp_written = [\"The project was completed.\", \"The car was driven.\"]\n",
        "getpp_spoken = [\"He got promoted to manager.\", \"The boy got caught stealing.\"]\n",
        "getpp_written = [\"She got invited to the party.\", \"They got accepted to the university.\"]\n",
        "\n",
        "# Extract past participles after 'be' and 'get'\n",
        "bepp_spoken_pp = extract_past_participles(bepp_spoken, ['be'])\n",
        "bepp_written_pp = extract_past_participles(bepp_written, ['be'])\n",
        "getpp_spoken_pp = extract_past_participles(getpp_spoken, ['get'])\n",
        "getpp_written_pp = extract_past_participles(getpp_written, ['get'])\n",
        "\n",
        "# Output the extracted past participles\n",
        "print(\"BePP Spoken Past Participles:\", bepp_spoken_pp)\n",
        "print(\"BePP Written Past Participles:\", bepp_written_pp)\n",
        "print(\"GetPP Spoken Past Participles:\", getpp_spoken_pp)\n",
        "print(\"GetPP Written Past Participles:\", getpp_written_pp)\n"
      ],
      "metadata": {
        "id": "_8qxJt2qdSVU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}